{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Matthew-Jennings/mech-interp-explore/blob/main/capital_letter_follows_full_stop_CONTINUED.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does GPT-2 Small Predict Capital Letters After Full Stops? - *Exploratory Analysis*\n",
    "### Matthew Jennings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pprint import pprint\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import einops\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import torch\n",
    "from circuitsvis.attention import attention_heads\n",
    "from fancy_einsum import einsum\n",
    "from IPython.display import HTML, IFrame\n",
    "from jaxtyping import Float\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens import ActivationCache, HookedTransformer\n",
    "\n",
    "from helpers import cumul_probs_by_capitalisation_type, correct_incorrect_answers_for_top_spacetitleword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x13fcd46b190>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define plotting helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor, **kwargs):\n",
    "    px.imshow(\n",
    "        utils.to_numpy(tensor),\n",
    "        color_continuous_midpoint=0.0,\n",
    "        color_continuous_scale=\"RdBu\",\n",
    "        **kwargs,\n",
    "    ).show()\n",
    "\n",
    "\n",
    "def line(tensor, **kwargs):\n",
    "    px.line(\n",
    "        y=utils.to_numpy(tensor),\n",
    "        **kwargs,\n",
    "    ).show()\n",
    "\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(\n",
    "        y=y,\n",
    "        x=x,\n",
    "        labels={\"x\": xaxis, \"y\": yaxis, \"color\": caxis},\n",
    "        **kwargs,\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Pytorch device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\Workspace\\arena3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1150: UserWarning: expandable_segments not supported on this platform (Triggered internally at ..\\c10/cuda/CUDAAllocatorConfig.h:30.)\n",
      "  return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    ")\n",
    "\n",
    "# Get the default device used\n",
    "device: torch.device = utils.get_device()\n",
    "print(f\"Pytorch device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Investigations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple first example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the model correctly predict capital letters after a full stop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'This', ' is', ' a', ' sentence', '.']\n",
      "Tokenized answer: [' It']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.14</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.67</span><span style=\"font-weight: bold\">% Token: | It|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m15.14\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m10.67\u001b[0m\u001b[1m% Token: | It|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 15.64 Prob: 17.54% Token: |\n",
      "|\n",
      "Top 1th token. Logit: 15.14 Prob: 10.67% Token: | It|\n",
      "Top 2th token. Logit: 14.47 Prob:  5.44% Token: | This|\n",
      "Top 3th token. Logit: 14.39 Prob:  5.03% Token: | I|\n",
      "Top 4th token. Logit: 14.37 Prob:  4.94% Token: | The|\n",
      "Top 5th token. Logit: 14.09 Prob:  3.75% Token: | If|\n",
      "Top 6th token. Logit: 14.07 Prob:  3.68% Token: | A|\n",
      "Top 7th token. Logit: 13.80 Prob:  2.79% Token: | You|\n",
      "Top 8th token. Logit: 13.26 Prob:  1.63% Token: | Please|\n",
      "Top 9th token. Logit: 12.99 Prob:  1.25% Token: | We|\n",
      "Top 10th token. Logit: 12.97 Prob:  1.22% Token: | In|\n",
      "Top 11th token. Logit: 12.87 Prob:  1.10% Token: | There|\n",
      "Top 12th token. Logit: 12.83 Prob:  1.06% Token: |\n",
      "\n",
      "|\n",
      "Top 13th token. Logit: 12.71 Prob:  0.94% Token: | For|\n",
      "Top 14th token. Logit: 12.66 Prob:  0.90% Token: | An|\n",
      "Top 15th token. Logit: 12.45 Prob:  0.73% Token: | See|\n",
      "Top 16th token. Logit: 12.38 Prob:  0.67% Token: |<|endoftext|>|\n",
      "Top 17th token. Logit: 12.26 Prob:  0.60% Token: | \"|\n",
      "Top 18th token. Logit: 12.24 Prob:  0.59% Token: | To|\n",
      "Top 19th token. Logit: 12.19 Prob:  0.56% Token: | As|\n",
      "Top 20th token. Logit: 12.13 Prob:  0.53% Token: | Read|\n",
      "Top 21th token. Logit: 12.02 Prob:  0.47% Token: | When|\n",
      "Top 22th token. Logit: 11.98 Prob:  0.46% Token: | Do|\n",
      "Top 23th token. Logit: 11.98 Prob:  0.45% Token: | That|\n",
      "Top 24th token. Logit: 11.94 Prob:  0.44% Token: | Let|\n",
      "Top 25th token. Logit: 11.93 Prob:  0.43% Token: | (|\n",
      "Top 26th token. Logit: 11.93 Prob:  0.43% Token: | No|\n",
      "Top 27th token. Logit: 11.86 Prob:  0.40% Token: | Your|\n",
      "Top 28th token. Logit: 11.82 Prob:  0.39% Token: | And|\n",
      "Top 29th token. Logit: 11.78 Prob:  0.37% Token: | What|\n",
      "Top 30th token. Logit: 11.75 Prob:  0.36% Token: | Don|\n",
      "Top 31th token. Logit: 11.67 Prob:  0.33% Token: | Here|\n",
      "Top 32th token. Logit: 11.53 Prob:  0.29% Token: | But|\n",
      "Top 33th token. Logit: 11.49 Prob:  0.28% Token: | All|\n",
      "Top 34th token. Logit: 11.47 Prob:  0.27% Token: | Be|\n",
      "Top 35th token. Logit: 11.46 Prob:  0.27% Token: | So|\n",
      "Top 36th token. Logit: 11.38 Prob:  0.25% Token: | Click|\n",
      "Top 37th token. Logit: 11.38 Prob:  0.25% Token: | Just|\n",
      "Top 38th token. Logit: 11.37 Prob:  0.25% Token: | Not|\n",
      "Top 39th token. Logit: 11.29 Prob:  0.23% Token: | Some|\n",
      "Top 40th token. Logit: 11.25 Prob:  0.22% Token: | Like|\n",
      "Top 41th token. Logit: 11.25 Prob:  0.22% Token: | One|\n",
      "Top 42th token. Logit: 11.23 Prob:  0.21% Token: | Go|\n",
      "Top 43th token. Logit: 11.21 Prob:  0.21% Token: | Each|\n",
      "Top 44th token. Logit: 11.20 Prob:  0.21% Token: | My|\n",
      "Top 45th token. Logit: 11.20 Prob:  0.21% Token: | First|\n",
      "Top 46th token. Logit: 11.14 Prob:  0.20% Token: | He|\n",
      "Top 47th token. Logit: 11.05 Prob:  0.18% Token: | Write|\n",
      "Top 48th token. Logit: 11.05 Prob:  0.18% Token: | Every|\n",
      "Top 49th token. Logit: 11.05 Prob:  0.18% Token: | Remember|\n",
      "Top 50th token. Logit: 11.02 Prob:  0.17% Token: | Take|\n",
      "Top 51th token. Logit: 11.02 Prob:  0.17% Token: | These|\n",
      "Top 52th token. Logit: 11.02 Prob:  0.17% Token: | Try|\n",
      "Top 53th token. Logit: 11.00 Prob:  0.17% Token: | Any|\n",
      "Top 54th token. Logit: 10.98 Prob:  0.17% Token: | While|\n",
      "Top 55th token. Logit: 10.98 Prob:  0.17% Token: | Its|\n",
      "Top 56th token. Logit: 10.98 Prob:  0.17% Token: | Because|\n",
      "Top 57th token. Logit: 10.97 Prob:  0.17% Token: | Think|\n",
      "Top 58th token. Logit: 10.89 Prob:  0.15% Token: | At|\n",
      "Top 59th token. Logit: 10.87 Prob:  0.15% Token: | After|\n",
      "Top 60th token. Logit: 10.79 Prob:  0.14% Token: | On|\n",
      "Top 61th token. Logit: 10.78 Prob:  0.14% Token: | Yes|\n",
      "Top 62th token. Logit: 10.75 Prob:  0.13% Token: | Get|\n",
      "Top 63th token. Logit: 10.75 Prob:  0.13% Token: | Feel|\n",
      "Top 64th token. Logit: 10.75 Prob:  0.13% Token: | Use|\n",
      "Top 65th token. Logit: 10.74 Prob:  0.13% Token: | By|\n",
      "Top 66th token. Logit: 10.73 Prob:  0.13% Token: | Make|\n",
      "Top 67th token. Logit: 10.72 Prob:  0.13% Token: | Or|\n",
      "Top 68th token. Logit: 10.71 Prob:  0.13% Token: | Although|\n",
      "Top 69th token. Logit: 10.71 Prob:  0.13% Token: | Have|\n",
      "Top 70th token. Logit: 10.69 Prob:  0.13% Token: | Find|\n",
      "Top 71th token. Logit: 10.69 Prob:  0.13% Token: | Once|\n",
      "Top 72th token. Logit: 10.69 Prob:  0.13% Token: | Words|\n",
      "Top 73th token. Logit: 10.66 Prob:  0.12% Token: | Nothing|\n",
      "Top 74th token. Logit: 10.64 Prob:  0.12% Token: | Sometimes|\n",
      "Top 75th token. Logit: 10.62 Prob:  0.12% Token: | Look|\n",
      "Top 76th token. Logit: 10.62 Prob:  0.12% Token: | Now|\n",
      "Top 77th token. Logit: 10.60 Prob:  0.11% Token: | From|\n",
      "Top 78th token. Logit: 10.57 Prob:  0.11% Token: | How|\n",
      "Top 79th token. Logit: 10.57 Prob:  0.11% Token: | [|\n",
      "Top 80th token. Logit: 10.55 Prob:  0.11% Token: | Learn|\n",
      "Top 81th token. Logit: 10.53 Prob:  0.11% Token: | Sorry|\n",
      "Top 82th token. Logit: 10.53 Prob:  0.11% Token: | Only|\n",
      "Top 83th token. Logit: 10.51 Prob:  0.10% Token: | Many|\n",
      "Top 84th token. Logit: 10.50 Prob:  0.10% Token: | Most|\n",
      "Top 85th token. Logit: 10.49 Prob:  0.10% Token: | Thank|\n",
      "Top 86th token. Logit: 10.44 Prob:  0.10% Token: | Note|\n",
      "Top 87th token. Logit: 10.41 Prob:  0.09% Token: | However|\n",
      "Top 88th token. Logit: 10.39 Prob:  0.09% Token: | Keep|\n",
      "Top 89th token. Logit: 10.39 Prob:  0.09% Token: | Follow|\n",
      "Top 90th token. Logit: 10.39 Prob:  0.09% Token: | Our|\n",
      "Top 91th token. Logit: 10.38 Prob:  0.09% Token: | Why|\n",
      "Top 92th token. Logit: 10.37 Prob:  0.09% Token: | Even|\n",
      "Top 93th token. Logit: 10.35 Prob:  0.09% Token: | Also|\n",
      "Top 94th token. Logit: 10.35 Prob:  0.09% Token: | Someone|\n",
      "Top 95th token. Logit: 10.32 Prob:  0.09% Token: | They|\n",
      "Top 96th token. Logit: 10.29 Prob:  0.08% Token: | Unless|\n",
      "Top 97th token. Logit: 10.27 Prob:  0.08% Token: | Is|\n",
      "Top 98th token. Logit: 10.25 Prob:  0.08% Token: | Something|\n",
      "Top 99th token. Logit: 10.23 Prob:  0.08% Token: | Perhaps|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' It'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' It'\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_prompt = \"This is a sentence.\"\n",
    "answer = \" It\"\n",
    "\n",
    "utils.test_prompt(example_prompt, answer, model, top_k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative probabilities for next token by 'pattern' for typical sentence:\n",
      "Space, First Char Uppercase: 77.78%\n",
      "Other: 21.19%\n",
      "No Space, First Char Uppercase: 0.35%\n",
      "Space, First Char Lowercase: 0.32%\n",
      "Space, First Char Numeral: 0.23%\n",
      "No Space, First Char Lowercase: 0.10%\n",
      "No Space, First Char Numeral: 0.03%\n",
      "\n",
      "Cumulative probabilities for next token by 'pattern' for all lowercase sentence:\n",
      "Space, First Char Uppercase: 42.14%\n",
      "Space, First Char Lowercase: 32.01%\n",
      "Other: 19.68%\n",
      "No Space, First Char Lowercase: 4.22%\n",
      "No Space, First Char Uppercase: 1.20%\n",
      "Space, First Char Numeral: 0.59%\n",
      "No Space, First Char Numeral: 0.15%\n",
      "\n",
      "Cumulative probabilities for next token by 'pattern' for all uppercase sentence:\n",
      "Space, First Char Uppercase: 75.63%\n",
      "Other: 21.66%\n",
      "No Space, First Char Uppercase: 2.00%\n",
      "Space, First Char Numeral: 0.40%\n",
      "Space, First Char Lowercase: 0.21%\n",
      "No Space, First Char Numeral: 0.06%\n",
      "No Space, First Char Lowercase: 0.03%\n"
     ]
    }
   ],
   "source": [
    "print(\"Cumulative probabilities for next token by 'pattern' for typical sentence:\")\n",
    "logits, _ = model.run_with_cache(example_prompt, remove_batch_dim=True)\n",
    "cumul_probs_by_capitalisation_type(logits[:, -1], model, print_=True)\n",
    "\n",
    "print(\"\\nCumulative probabilities for next token by 'pattern' for all lowercase sentence:\")\n",
    "logits, _ = model.run_with_cache(example_prompt.lower(), remove_batch_dim=True)\n",
    "cumul_probs_by_capitalisation_type(logits[:, -1], model, print_=True)\n",
    "\n",
    "print(\"\\nCumulative probabilities for next token by 'pattern' for all uppercase sentence:\")\n",
    "logits, _ = model.run_with_cache(example_prompt.upper(), remove_batch_dim=True)\n",
    "cumul_probs_by_capitalisation_type(logits[:, -1], model, print_=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "- Interesting! The top result is a newline. Inspecting the rest of the results provides a nice reminder that tokens of the form `<space><capital-letter>` *are not only valid tokens to follow a full stop.* Other examples include:\n",
    "  - `\"\\n\"`\n",
    "  - `\"\\n\\n\"`\n",
    "  - `\"<|endoftext|>\"`\n",
    "  - Non-alphanumeric chars (preceded by spaces): ` (`, ` \"`\n",
    "\n",
    "- If we treat any answer of the forms `<space><titleword>` and `Other` as valid, then, a basic sampling of the next token will produce a valid token $77.8 + 21.2 = 99\\%$ of the time.\n",
    "\n",
    "- The same prompt adjusted to all *lowercase* letters - particularly the first letter of the first word - significantly raises the probability that a token of the form `<space><lowercase_word>` would be basic sampled from 0.32% to 32% - a 100x increase!\n",
    "\n",
    "- The same prompt adjusted to all *uppercase* letters results in a similar probability distribution by token pattern as the original prompt, albeit tokens of the form `<no_space><titleword>` increase in probaility from 0.35% to 2%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'The', ' probability', ' of', ' me', ' getting', ' to', ' the', ' bottom', ' of', ' this', ' circuit', ' is', ' not', ' 0', '.']\n",
      "Tokenized answer: [' It']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.33</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.20</span><span style=\"font-weight: bold\">% Token: | It|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m9\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m15.33\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m2.20\u001b[0m\u001b[1m% Token: | It|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 16.66 Prob:  8.34% Token: |\n",
      "|\n",
      "Top 1th token. Logit: 16.62 Prob:  7.99% Token: |5|\n",
      "Top 2th token. Logit: 16.19 Prob:  5.20% Token: |1|\n",
      "Top 3th token. Logit: 15.69 Prob:  3.16% Token: |01|\n",
      "Top 4th token. Logit: 15.61 Prob:  2.91% Token: | The|\n",
      "Top 5th token. Logit: 15.59 Prob:  2.87% Token: |001|\n",
      "Top 6th token. Logit: 15.58 Prob:  2.84% Token: |0001|\n",
      "Top 7th token. Logit: 15.56 Prob:  2.77% Token: | If|\n",
      "Top 8th token. Logit: 15.55 Prob:  2.76% Token: |0|\n",
      "Top 9th token. Logit: 15.33 Prob:  2.20% Token: | It|\n",
      "Top 10th token. Logit: 15.13 Prob:  1.81% Token: |9|\n",
      "Top 11th token. Logit: 15.08 Prob:  1.72% Token: | But|\n",
      "Top 12th token. Logit: 14.95 Prob:  1.51% Token: |000|\n",
      "Top 13th token. Logit: 14.95 Prob:  1.51% Token: |6|\n",
      "Top 14th token. Logit: 14.94 Prob:  1.49% Token: |2|\n",
      "Top 15th token. Logit: 14.87 Prob:  1.39% Token: |25|\n",
      "Top 16th token. Logit: 14.81 Prob:  1.32% Token: |05|\n",
      "Top 17th token. Logit: 14.81 Prob:  1.32% Token: |8|\n",
      "Top 18th token. Logit: 14.81 Prob:  1.31% Token: | I|\n",
      "Top 19th token. Logit: 14.80 Prob:  1.30% Token: |00|\n",
      "Top 20th token. Logit: 14.72 Prob:  1.21% Token: |7|\n",
      "Top 21th token. Logit: 14.66 Prob:  1.13% Token: |0000|\n",
      "Top 22th token. Logit: 14.66 Prob:  1.13% Token: |3|\n",
      "Top 23th token. Logit: 14.53 Prob:  1.00% Token: | This|\n",
      "Top 24th token. Logit: 14.52 Prob:  0.98% Token: |75|\n",
      "Top 25th token. Logit: 14.51 Prob:  0.97% Token: | However|\n",
      "Top 26th token. Logit: 14.49 Prob:  0.95% Token: |4|\n",
      "Top 27th token. Logit: 14.42 Prob:  0.89% Token: | You|\n",
      "Top 28th token. Logit: 14.32 Prob:  0.80% Token: |99|\n",
      "Top 29th token. Logit: 14.26 Prob:  0.76% Token: | In|\n",
      "Top 30th token. Logit: 14.12 Prob:  0.66% Token: | That|\n",
      "Top 31th token. Logit: 14.07 Prob:  0.63% Token: |000000|\n",
      "Top 32th token. Logit: 13.95 Prob:  0.55% Token: | There|\n",
      "Top 33th token. Logit: 13.90 Prob:  0.53% Token: |9999|\n",
      "Top 34th token. Logit: 13.88 Prob:  0.52% Token: |02|\n",
      "Top 35th token. Logit: 13.88 Prob:  0.52% Token: |002|\n",
      "Top 36th token. Logit: 13.78 Prob:  0.47% Token: |00000000|\n",
      "Top 37th token. Logit: 13.75 Prob:  0.46% Token: |005|\n",
      "Top 38th token. Logit: 13.72 Prob:  0.44% Token: | So|\n",
      "Top 39th token. Logit: 13.59 Prob:  0.39% Token: |\n",
      "\n",
      "|\n",
      "Top 40th token. Logit: 13.56 Prob:  0.38% Token: |50|\n",
      "Top 41th token. Logit: 13.55 Prob:  0.37% Token: |10|\n",
      "Top 42th token. Logit: 13.48 Prob:  0.35% Token: | A|\n",
      "Top 43th token. Logit: 13.47 Prob:  0.35% Token: | For|\n",
      "Top 44th token. Logit: 13.42 Prob:  0.33% Token: |06|\n",
      "Top 45th token. Logit: 13.37 Prob:  0.31% Token: | We|\n",
      "Top 46th token. Logit: 13.34 Prob:  0.30% Token: |04|\n",
      "Top 47th token. Logit: 13.33 Prob:  0.30% Token: |999|\n",
      "Top 48th token. Logit: 13.28 Prob:  0.28% Token: |09|\n",
      "Top 49th token. Logit: 13.26 Prob:  0.28% Token: | Therefore|\n",
      "Top 50th token. Logit: 13.24 Prob:  0.27% Token: |<|endoftext|>|\n",
      "Top 51th token. Logit: 13.23 Prob:  0.27% Token: | As|\n",
      "Top 52th token. Logit: 13.21 Prob:  0.27% Token: |95|\n",
      "Top 53th token. Logit: 13.14 Prob:  0.25% Token: | (|\n",
      "Top 54th token. Logit: 13.14 Prob:  0.25% Token: | And|\n",
      "Top 55th token. Logit: 13.07 Prob:  0.23% Token: |07|\n",
      "Top 56th token. Logit: 13.05 Prob:  0.23% Token: |08|\n",
      "Top 57th token. Logit: 13.04 Prob:  0.22% Token: |03|\n",
      "Top 58th token. Logit: 13.02 Prob:  0.22% Token: |15|\n",
      "Top 59th token. Logit: 13.00 Prob:  0.22% Token: | One|\n",
      "Top 60th token. Logit: 12.95 Prob:  0.20% Token: |0000000000000000|\n",
      "Top 61th token. Logit: 12.94 Prob:  0.20% Token: |35|\n",
      "Top 62th token. Logit: 12.88 Prob:  0.19% Token: |33|\n",
      "Top 63th token. Logit: 12.87 Prob:  0.19% Token: |85|\n",
      "Top 64th token. Logit: 12.84 Prob:  0.18% Token: | When|\n",
      "Top 65th token. Logit: 12.83 Prob:  0.18% Token: |500|\n",
      "Top 66th token. Logit: 12.82 Prob:  0.18% Token: |0002|\n",
      "Top 67th token. Logit: 12.82 Prob:  0.18% Token: |618|\n",
      "Top 68th token. Logit: 12.82 Prob:  0.18% Token: |45|\n",
      "Top 69th token. Logit: 12.77 Prob:  0.17% Token: |025|\n",
      "Top 70th token. Logit: 12.76 Prob:  0.17% Token: | To|\n",
      "Top 71th token. Logit: 12.76 Prob:  0.17% Token: | What|\n",
      "Top 72th token. Logit: 12.75 Prob:  0.17% Token: | On|\n",
      "Top 73th token. Logit: 12.73 Prob:  0.16% Token: | 1|\n",
      "Top 74th token. Logit: 12.73 Prob:  0.16% Token: |00000|\n",
      "Top 75th token. Logit: 12.70 Prob:  0.16% Token: |97|\n",
      "Top 76th token. Logit: 12.69 Prob:  0.16% Token: |007|\n",
      "Top 77th token. Logit: 12.69 Prob:  0.16% Token: |67|\n",
      "Top 78th token. Logit: 12.60 Prob:  0.14% Token: | At|\n",
      "Top 79th token. Logit: 12.58 Prob:  0.14% Token: |12|\n",
      "Top 80th token. Logit: 12.53 Prob:  0.13% Token: |13|\n",
      "Top 81th token. Logit: 12.51 Prob:  0.13% Token: | Even|\n",
      "Top 82th token. Logit: 12.50 Prob:  0.13% Token: |006|\n",
      "Top 83th token. Logit: 12.48 Prob:  0.13% Token: | No|\n",
      "Top 84th token. Logit: 12.46 Prob:  0.12% Token: | Not|\n",
      "Top 85th token. Logit: 12.45 Prob:  0.12% Token: | .|\n",
      "Top 86th token. Logit: 12.44 Prob:  0.12% Token: | Then|\n",
      "Top 87th token. Logit: 12.44 Prob:  0.12% Token: |11|\n",
      "Top 88th token. Logit: 12.44 Prob:  0.12% Token: |49|\n",
      "Top 89th token. Logit: 12.41 Prob:  0.12% Token: |003|\n",
      "Top 90th token. Logit: 12.37 Prob:  0.11% Token: |37|\n",
      "Top 91th token. Logit: 12.37 Prob:  0.11% Token: | Because|\n",
      "Top 92th token. Logit: 12.34 Prob:  0.11% Token: | By|\n",
      "Top 93th token. Logit: 12.31 Prob:  0.11% Token: | Of|\n",
      "Top 94th token. Logit: 12.28 Prob:  0.10% Token: |20|\n",
      "Top 95th token. Logit: 12.28 Prob:  0.10% Token: |77|\n",
      "Top 96th token. Logit: 12.24 Prob:  0.10% Token: |27|\n",
      "Top 97th token. Logit: 12.23 Prob:  0.10% Token: |100|\n",
      "Top 98th token. Logit: 12.21 Prob:  0.10% Token: |87|\n",
      "Top 99th token. Logit: 12.21 Prob:  0.10% Token: |30|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' It'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' It'\u001b[0m, \u001b[1;36m9\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cumulative probabilities for next token by 'pattern' for typical sentence ending in the numeral 0:\n",
      "No Space, First Char Numeral: 61.18%\n",
      "Space, First Char Uppercase: 27.35%\n",
      "Other: 10.12%\n",
      "Space, First Char Numeral: 0.72%\n",
      "No Space, First Char Uppercase: 0.30%\n",
      "Space, First Char Lowercase: 0.27%\n",
      "No Space, First Char Lowercase: 0.06%\n"
     ]
    }
   ],
   "source": [
    "numeric_example_prompt = (\n",
    "    \"The probability of me getting to the bottom of this circuit is not 0.\"\n",
    ")\n",
    "\n",
    "utils.test_prompt(numeric_example_prompt, answer, model, top_k=100)\n",
    "print(\"\\nCumulative probabilities for next token by 'pattern' for typical sentence ending in the numeral 0:\")\n",
    "logits, _ = model.run_with_cache(numeric_example_prompt, remove_batch_dim=True)\n",
    "cumul_probs_by_capitalisation_type(logits[:, -1], model, print_=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- As with the original sentence, the top predicted token for this new example sentence with a numeric final character is a newline character!\n",
    "\n",
    "- The next highest prediction `5`, with no preceding space. Interesting that a token of the form `<space><titleword>` is *not* more likely.\n",
    "\n",
    "- Of the top 20 predictions, only 5 are non-numeric (one of which is a newline).\n",
    "\n",
    "- Valid answers of the form `<no_space><numeral>`, `<space><titleword>` and \"Other\" will be basic sampled with just under 99% probability. An numeric, immediately next character is most likely, at 61%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Rigorous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate multiple, very simple prompts.\n",
    "- Single sentence.\n",
    "- Same number of tokens.\n",
    "- No proper nouns\n",
    "- No other punctuation\n",
    "- No non-alphabetical characters other than spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 9, 9, 9, 9, 9, 9]\n"
     ]
    }
   ],
   "source": [
    "prompts_simple = [\n",
    "    \"This is a very slightly longer sentence.\",\n",
    "    \"The cat loves to scratch my car.\",\n",
    "    \"Kangaroos should be ridden.\",\n",
    "    \"That car driver drives rather recklessly.\",\n",
    "    \"It is wonderful to be alive today.\",\n",
    "    \"This is a difficult problem to solve.\", \n",
    "    \"Koalas are grumpy cute.\",\n",
    "]\n",
    "prompt_token_counts = [len(model.to_str_tokens(prompt)) for prompt in prompts_simple]\n",
    "print(prompt_token_counts)\n",
    "assert all(count == prompt_token_counts[0] for count in prompt_token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' I', ' i', 'I', 'i'), (' It', ' it', 'It', 'it'), (' They', ' they', 'They', 'they'), (' He', ' he', 'He', 'he'), (' I', ' i', 'I', 'i'), (' The', ' the', 'The', 'the'), (' They', ' they', 'They', 'they')]\n"
     ]
    }
   ],
   "source": [
    "logits, _ = model.run_with_cache(prompts_simple)\n",
    "\n",
    "logits_final_sorted, logits_final_sorted_idx = logits[:, -1, :].sort(dim=-1, descending=True)\n",
    "\n",
    "answer_tokens, answer_str_tokens = correct_incorrect_answers_for_top_spacetitleword(logits_final_sorted_idx, model)\n",
    "\n",
    "print(answer_str_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate new answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "answer_tokens, answer_str_tokens = titleword_answer_generator(logits_idx_sorted)\n",
    "pprint(answer_str_tokens)\n",
    "print(answer_tokens)\n",
    "\n",
    "[' Go', '\\n', '\\n', ' He', ' I', ' It', '\\n', '\\n']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New logit diffs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_incorrect_answer_label = dict(\n",
    "    (\n",
    "        (1, \"lowercase\"),\n",
    "        (2, \"missing space\"),\n",
    "        (3, \"lowercase and missing space\"),\n",
    "    )\n",
    ")\n",
    "print(\"Prompts:\")\n",
    "for prompt in prompts:\n",
    "    print(f\"'{prompt}'\")\n",
    "\n",
    "print(f\"\\nTop space-titleword prediction per prompt:\\n{[row[0] for row in answer_str_tokens]}\")\n",
    "for incorrect_idx in range(1, answer_tokens.size(1)):\n",
    "    print(\n",
    "        f\"\\nLogit diffs for incorrect answer type: '{idx_to_incorrect_answer_label[incorrect_idx]}':\"\n",
    "    )\n",
    "    logit_diffs = logits_to_ave_logit_diff_2(\n",
    "        logits, answer_tokens, per_prompt=True, incorrect_idx=incorrect_idx\n",
    "    )\n",
    "\n",
    "\"\"\"Logit diffs for incorrect answer type: 'lowercase':\n",
    "Per prompt logit difference: tensor([5.5300, 5.4780, 5.8090, 6.1450, 6.2310, 7.7480, 8.2100, 6.4700])\n",
    "Average logit difference: 6.453\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "- All values high now. Avoid sentences that are expected not to conform to standard English prose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_idx = 1\n",
    "\n",
    "original_average_logit_diff = logits_to_ave_logit_diff_2(logits, answer_tokens, per_prompt=False, incorrect_idx=incorrect_idx, print_=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_residual_directions = model.tokens_to_residual_directions(answer_tokens)\n",
    "print(\"Answer residual directions shape:\", answer_residual_directions.shape)\n",
    "\n",
    "logit_diff_directions = (\n",
    "    answer_residual_directions[:, 0] - answer_residual_directions[:, incorrect_idx]\n",
    ")\n",
    "print(\"Logit difference directions shape:\", logit_diff_directions.shape)\n",
    "\n",
    "# cache syntax - resid_post is the residual stream at the end of the layer, -1 gets the final layer. The general syntax is [activation_name, layer_index, sub_layer_type].\n",
    "final_residual_stream = cache[\"resid_post\", -1]\n",
    "print(\"Final residual stream shape:\", final_residual_stream.shape)\n",
    "final_token_residual_stream = final_residual_stream[:, -1, :]\n",
    "# Apply LayerNorm scaling\n",
    "# pos_slice is the subset of the positions we take - here the final token of each prompt\n",
    "scaled_final_token_residual_stream = cache.apply_ln_to_stack(\n",
    "    final_token_residual_stream, layer=-1, pos_slice=-1\n",
    ")\n",
    "\n",
    "average_logit_diff = einsum(\n",
    "    \"batch d_model, batch d_model -> \",\n",
    "    scaled_final_token_residual_stream,\n",
    "    logit_diff_directions,\n",
    ") / len(prompts)\n",
    "print(\"Calculated average logit diff:\", round(average_logit_diff.item(), 3))\n",
    "print(\"Original logit difference:\", round(original_average_logit_diff.item(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Still different.\n",
    "- I think a possible cause is that I have not not been picking the top answer for each prompt\n",
    "as the correct answer.\n",
    "\n",
    "  - E.g., there may be some difference in scaling across the prompts that makes comparing\n",
    "an average invalid. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_answers = [\n",
    "    (\" Go\", \" go\"),\n",
    "    (\"\\n\", \" i\"),\n",
    "    (\"\\n\", \" i\"),\n",
    "    (\"\\n\", \" i\"),\n",
    "    (\" I\", \" i\"),\n",
    "    (\" It\", \" it\"),\n",
    "    (\"\\n\", \" i\"),\n",
    "    (\"\\n\", \" i\"),\n",
    "]\n",
    "\n",
    "top_answer_tokens = torch.tensor(\n",
    "    [[model.to_single_token(ans_row[0]), model.to_single_token(ans_row[1])] for ans_row in top_answers]\n",
    ").to(device)\n",
    "print(top_answer_tokens)\n",
    "logits_top_answer, cache_top_answer = model.run_with_cache(top_answer_tokens)\n",
    "original_average_logit_diff = logits_to_ave_logit_diff_2(logits_top_answer, top_answer_tokens, per_prompt=False, incorrect_idx=1, print_=False)\n",
    "\n",
    "answer_residual_directions = model.tokens_to_residual_directions(top_answer_tokens)\n",
    "print(\"Answer residual directions shape:\", answer_residual_directions.shape)\n",
    "\n",
    "top_answer_logit_diff_directions = (\n",
    "    answer_residual_directions[:, 0] - answer_residual_directions[:, 1]\n",
    ")\n",
    "print(\"Logit difference directions shape:\", top_answer_logit_diff_directions.shape)\n",
    "\n",
    "# cache syntax - resid_post is the residual stream at the end of the layer, -1 gets the final layer. The general syntax is [activation_name, layer_index, sub_layer_type].\n",
    "final_residual_stream = cache_top_answer[\"resid_post\", -1]\n",
    "print(\"Final residual stream shape:\", final_residual_stream.shape)\n",
    "final_token_residual_stream = final_residual_stream[:, -1, :]\n",
    "# Apply LayerNorm scaling\n",
    "# pos_slice is the subset of the positions we take - here the final token of each prompt\n",
    "scaled_final_token_residual_stream = cache_top_answer.apply_ln_to_stack(\n",
    "    final_token_residual_stream, layer=-1, pos_slice=-1\n",
    ")\n",
    "\n",
    "average_logit_diff = einsum(\n",
    "    \"batch d_model, batch d_model -> \",\n",
    "    scaled_final_token_residual_stream,\n",
    "    top_answer_logit_diff_directions,\n",
    ") / len(top_answers)\n",
    "print(\"Calculated average logit diff:\", round(average_logit_diff.item(), 3))\n",
    "print(\"Original logit difference:\", round(original_average_logit_diff.item(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm. Still no good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the interest of time, press on\n",
    "\n",
    "- In normal circumstances, I would seek advice from a colleague/mentor on cause/importance of this difference in scaling.\n",
    "\n",
    "- I have a small hunch that the difference may not overly adversely impact meaningfulness of results.\n",
    "\n",
    "- Press on as if no difference for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_average_logit_diff = logits_to_ave_logit_diff_2(logits, answer_tokens, per_prompt=False, incorrect_idx=1, print_=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_stack_to_logit_diff(\n",
    "    residual_stack: Float[torch.Tensor, \"components batch d_model\"],\n",
    "    cache: ActivationCache,\n",
    ") -> float:\n",
    "    scaled_residual_stack = cache.apply_ln_to_stack(\n",
    "        residual_stack, layer=-1, pos_slice=-1\n",
    "    )\n",
    "    return einsum(\n",
    "        \"... batch d_model, batch d_model -> ...\",\n",
    "        scaled_residual_stack,\n",
    "        logit_diff_directions,\n",
    "    ) / len(prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulated_residual, labels = cache.accumulated_resid(\n",
    "    layer=-1, incl_mid=True, pos_slice=-1, return_labels=True\n",
    ")\n",
    "logit_lens_logit_diffs = residual_stack_to_logit_diff(accumulated_residual, cache)\n",
    "line(\n",
    "    logit_lens_logit_diffs,\n",
    "    x=np.arange(model.cfg.n_layers * 2 + 1) / 2,\n",
    "    hover_name=labels,\n",
    "    title=\"Logit Difference From Accumulated Residual Stream\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_layer_residual, labels = cache.decompose_resid(\n",
    "    layer=-1, pos_slice=-1, return_labels=True\n",
    ")\n",
    "per_layer_logit_diffs = residual_stack_to_logit_diff(per_layer_residual, cache)\n",
    "line(per_layer_logit_diffs, hover_name=labels, title=\"Logit Difference From Each Layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "\n",
    "- It appears that MLP layers are primarily what matters.\n",
    "\n",
    "- Especially the later ones (with increasing effect toward the later layers)\n",
    "\n",
    "- With my limited understanding of transformers, this makes some sense. It seems to me that the most important information for predicting a space-titleword token next is the nature of the *current* position/token: it is a \"sentence terminating character\". Since MLP layers are used to process information at a position, I think this info is likely \"determined/used\" in MLP layera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head Attribution\n",
    "\n",
    "- Just out of interest, of course heads are parts of attention layers, which are apparently relatively unimportant - at least directly - according to the layer attribution graph above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_head_residual, labels = cache.stack_head_results(\n",
    "    layer=-1, pos_slice=-1, return_labels=True\n",
    ")\n",
    "per_head_logit_diffs = residual_stack_to_logit_diff(per_head_residual, cache)\n",
    "per_head_logit_diffs = einops.rearrange(\n",
    "    per_head_logit_diffs,\n",
    "    \"(layer head_index) -> layer head_index\",\n",
    "    layer=model.cfg.n_layers,\n",
    "    head_index=model.cfg.n_heads,\n",
    ")\n",
    "imshow(\n",
    "    per_head_logit_diffs,\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    "    title=\"Logit Difference From Each Head\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "- `0.1`, `10.7` and `11.0` look most interesting, in at least relative to the others.\n",
    "\n",
    "- Note the colormap  scale. Not much effect - at least as compared to IOI! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_patterns(\n",
    "    heads: Union[List[int], int, Float[torch.Tensor, \"heads\"]],\n",
    "    local_cache: ActivationCache,\n",
    "    local_tokens: torch.Tensor,\n",
    "    title: Optional[str] = \"\",\n",
    "    max_width: Optional[int] = 700,\n",
    ") -> str:\n",
    "    # If a single head is given, convert to a list\n",
    "    if isinstance(heads, int):\n",
    "        heads = [heads]\n",
    "\n",
    "    # Create the plotting data\n",
    "    labels: List[str] = []\n",
    "    patterns: List[Float[torch.Tensor, \"dest_pos src_pos\"]] = []\n",
    "\n",
    "    # Assume we have a single batch item\n",
    "    batch_index = 0\n",
    "\n",
    "    for head in heads:\n",
    "        # Set the label\n",
    "        layer = head // model.cfg.n_heads\n",
    "        head_index = head % model.cfg.n_heads\n",
    "        labels.append(f\"L{layer}H{head_index}\")\n",
    "\n",
    "        # Get the attention patterns for the head\n",
    "        # Attention patterns have shape [batch, head_index, query_pos, key_pos]\n",
    "        patterns.append(local_cache[\"attn\", layer][batch_index, head_index])\n",
    "\n",
    "    # Convert the tokens to strings (for the axis labels)\n",
    "    str_tokens = model.to_str_tokens(local_tokens)\n",
    "\n",
    "    # Combine the patterns into a single tensor\n",
    "    patterns: Float[torch.Tensor, \"head_index dest_pos src_pos\"] = torch.stack(\n",
    "        patterns, dim=0\n",
    "    )\n",
    "\n",
    "    # Circuitsvis Plot (note we get the code version so we can concatenate with the title)\n",
    "    plot = attention_heads(\n",
    "        attention=patterns, tokens=str_tokens, attention_head_names=labels\n",
    "    ).show_code()\n",
    "\n",
    "    # Display the title\n",
    "    title_html = f\"<h2>{title}</h2><br/>\"\n",
    "\n",
    "    # Return the visualisation as raw code\n",
    "    return f\"<div style='max-width: {str(max_width)}px;'>{title_html + plot}</div>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 3\n",
    "\n",
    "top_positive_logit_attr_heads = torch.topk(\n",
    "    per_head_logit_diffs.flatten(), k=top_k\n",
    ").indices\n",
    "\n",
    "positive_html = visualize_attention_patterns(\n",
    "    top_positive_logit_attr_heads,\n",
    "    cache,\n",
    "    tokens[0],\n",
    "    f\"Top {top_k} Positive Logit Attribution Heads\",\n",
    ")\n",
    "\n",
    "top_negative_logit_attr_heads = torch.topk(\n",
    "    -per_head_logit_diffs.flatten(), k=top_k\n",
    ").indices\n",
    "\n",
    "negative_html = visualize_attention_patterns(\n",
    "    top_negative_logit_attr_heads,\n",
    "    cache,\n",
    "    tokens[0],\n",
    "    title=f\"Top {top_k} Negative Logit Attribution Heads\",\n",
    ")\n",
    "\n",
    "HTML(positive_html + negative_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "- In the strongest head, `11.0`, token `1` attends strongly (relatively speaking) to token `0` (`<|endoftext|>`). This might contribute to capitalisation of token `1`?\n",
    "\n",
    "- In `0.1`, the final token (`\".\"`) attends fairly strongly to token `7`, which is another full stop in at least one of the prompts.\n",
    "\n",
    "- In `10.7`, attention to the second to last token negatively contributes to the logits of the final token.\n",
    "  - One (major?) flaw in this analysis is investigating only the first titleword token. For the selection of prompts I chose, perhaps the token immediately preceding the final full stop contributes to a stronger prediction of some other token (the newline?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Stream Patching\n",
    "\n",
    "- **Note**: It's difficult to for me to know how much wisdom I can draw from this section. The \"corrupted\" prompts below do significantly increase the logits for the \\<space\\>\\<**lowercase**\\> version of the next token, but they do not meaningfully *reduce* the logits of the \"clean\" token. It is not a neat reversal like the IOI example\n",
    "\n",
    "- The problem is: I'm struggling to think of a way to manufacture such a reversal, especially using the *relevant circuitry*. I'm sure I am missing something, but this seems like a difficult problem to provide an input that tests the \"capitalise next word after full stop\" circuit for which I can provide a corrupted input that produces an uncapitalised first word.\n",
    "  - E.g., perhaps I could exclusively use **repeat sequences** of a single word (e.g., clean version:`[\"Go. Go. Go. Go\"]`, corrupted version:`[\"go. go. go. go\"]`), but it seems likely that some *different circuit* (e.g., duplication) produces capitalised output in the clean case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_corrupted = [\n",
    "    \"filling up to eleven tokens.\\ngo.\", \n",
    "    \"hello. hello. hello. hello. hello.\", \n",
    "    \"yeah Matt doesn't know what he is doing.\", \n",
    "    \"that Will does not know where he is going.\",\n",
    "    \"someone should really help them out, I think.\", \n",
    "    \"it is wonderful to be in Adelaide in March.\",\n",
    "    \"koalas are cute, but grumpy.\", \n",
    "    \"this is a sentence. this is another sentence.\",\n",
    "]\n",
    "\n",
    "tokens_corrupted = model.to_tokens(prompts_corrupted)\n",
    "\n",
    "logits_corrupted, cache_corrupted = model.run_with_cache(tokens_corrupted)\n",
    "logits_final_corrupted = logits_corrupted[:, -1, :]\n",
    "logits_sorted_corrupted, logits_idx_sorted_corrupted = logits_final_corrupted.sort(\n",
    "    descending=True, stable=True, dim=-1\n",
    ")\n",
    "average_logit_diff_corrupted = logits_to_ave_logit_diff_2(logits_corrupted, answer_tokens, per_prompt=False, incorrect_idx=1, print_=True)\n",
    "print(\"\\nCorrupted Average Logit Diff\", round(average_logit_diff_corrupted.item(), 2))\n",
    "print(\"Clean Average Logit Diff\", round(original_average_logit_diff.item(), 2))\n",
    "utils.test_prompt(prompts_corrupted[1], answer_str_tokens[1][0], model, top_k=1)\n",
    "utils.test_prompt(prompts_corrupted[1], answer_str_tokens[1][1], model, top_k=1)\n",
    "\n",
    "    \"\"\"\n",
    "Per prompt logit difference: tensor([ 1.3570, -2.6480, -1.0820,  3.3580,  2.7450,  2.0550,  2.8190, -2.4030])\n",
    "Average logit difference: 0.775\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "\n",
    "- 5 out of 8 did not even reverse! But any positive logit differences are smaller.\n",
    "\n",
    "- That said, at least for these prompts, ***capitalisation of the next \"word-like\" token appears to depend fairly strongly on whether or not the *first* token (not token `0`/`<|endoftext|>`) was capitalised.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_residual_component(\n",
    "    corrupted_residual_component: Float[torch.Tensor, \"batch pos d_model\"],\n",
    "    hook,\n",
    "    pos,\n",
    "    clean_cache,\n",
    "):\n",
    "    corrupted_residual_component[:, pos, :] = clean_cache[hook.name][:, pos, :]\n",
    "    return corrupted_residual_component\n",
    "\n",
    "\n",
    "def normalize_patched_logit_diff(patched_logit_diff):\n",
    "    # Subtract corrupted logit diff to measure the improvement, divide by the total improvement from clean to corrupted to normalise\n",
    "    # 0 means zero change, negative means actively made worse, 1 means totally recovered clean performance, >1 means actively *improved* on clean performance\n",
    "    return (patched_logit_diff - average_logit_diff_corrupted) / (\n",
    "        original_average_logit_diff - average_logit_diff_corrupted\n",
    "    )\n",
    "\n",
    "\n",
    "patched_residual_stream_diff = torch.zeros(\n",
    "    model.cfg.n_layers, tokens.shape[1], device=device, dtype=torch.float32\n",
    ")\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    for position in range(tokens.shape[1]):\n",
    "        hook_fn = partial(patch_residual_component, pos=position, clean_cache=cache)\n",
    "        patched_logits = model.run_with_hooks(\n",
    "            tokens_corrupted,\n",
    "            fwd_hooks=[(utils.get_act_name(\"resid_pre\", layer), hook_fn)],\n",
    "            return_type=\"logits\",\n",
    "        )\n",
    "        patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)\n",
    "\n",
    "        patched_residual_stream_diff[layer, position] = normalize_patched_logit_diff(\n",
    "            patched_logit_diff\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_position_labels = [\n",
    "    f\"{tok}_{i}\" for i, tok in enumerate(model.to_str_tokens(tokens[0]))\n",
    "]\n",
    "imshow(\n",
    "    patched_residual_stream_diff,\n",
    "    x=prompt_position_labels,\n",
    "    title=\"Logit Difference From Patched Residual Stream\",\n",
    "    labels={\"x\": \"Position\", \"y\": \"Layer\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "\n",
    "- **With all of the above caveats**\n",
    "  - It *does* appear that some of the contributory computation is happening on the first token, which is ordinarily capitalised.\n",
    "\n",
    "  - At around layers 5-8, the information is moved to the final token?\n",
    "    - Are the initial layers determining a \"capitalisation-state\" datum for the first token in the sequence, and using this to help determine the token following the full stop?\n",
    "\n",
    "  - Not much other evidence for this though. Attention heads in these layers seem to weakly affect logits directly. Let's have a squiz anyway:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heads in layers 5 - 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 3\n",
    "\n",
    "top_positive_logit_attr_heads = torch.topk(\n",
    "    per_head_logit_diffs.flatten(), k=top_k\n",
    ").indices\n",
    "\n",
    "html = \"\"\n",
    "for layer in range(5, 9):\n",
    "    heads = range(layer*model.cfg.n_heads, (layer+1)*model.cfg.n_heads)\n",
    "\n",
    "    html += visualize_attention_patterns(\n",
    "        heads,\n",
    "        cache,\n",
    "        tokens[0],\n",
    "        f\"Layer {layer} Logit Attribution Heads\",\n",
    "    )\n",
    "\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "- Not seeing any evidence of attention paid to F by fullstop in layers 6-8...\n",
    "\n",
    "- Missing something."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
