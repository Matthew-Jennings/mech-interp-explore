{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does GPT-2 Small Predict Capital Letters After Full Stops? - *Exploratory Analysis*\n",
    "### Matthew Jennings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import einops\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import torch\n",
    "from circuitsvis.attention import attention_heads\n",
    "from fancy_einsum import einsum\n",
    "from IPython.display import HTML, IFrame\n",
    "from jaxtyping import Float\n",
    "\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens import ActivationCache, HookedTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x172d23a0b50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define plotting helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor, **kwargs):\n",
    "    px.imshow(\n",
    "        utils.to_numpy(tensor),\n",
    "        color_continuous_midpoint=0.0,\n",
    "        color_continuous_scale=\"RdBu\",\n",
    "        **kwargs,\n",
    "    ).show()\n",
    "\n",
    "\n",
    "def line(tensor, **kwargs):\n",
    "    px.line(\n",
    "        y=utils.to_numpy(tensor),\n",
    "        **kwargs,\n",
    "    ).show()\n",
    "\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(\n",
    "        y=y,\n",
    "        x=x,\n",
    "        labels={\"x\": xaxis, \"y\": yaxis, \"color\": caxis},\n",
    "        **kwargs,\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Pytorch device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\Workspace\\arena3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1150: UserWarning: expandable_segments not supported on this platform (Triggered internally at ..\\c10/cuda/CUDAAllocatorConfig.h:30.)\n",
      "  return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    ")\n",
    "\n",
    "# Get the default device used\n",
    "device: torch.device = utils.get_device()\n",
    "print(f\"Pytorch device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple first example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the model correctly predict capital letters after a full stop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'This', ' is', ' a', ' sentence', '.']\n",
      "Tokenized answer: [' It']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.14</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.67</span><span style=\"font-weight: bold\">% Token: | It|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m15.14\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m10.67\u001b[0m\u001b[1m% Token: | It|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 15.64 Prob: 17.54% Token: |\n",
      "|\n",
      "Top 1th token. Logit: 15.14 Prob: 10.67% Token: | It|\n",
      "Top 2th token. Logit: 14.47 Prob:  5.44% Token: | This|\n",
      "Top 3th token. Logit: 14.39 Prob:  5.03% Token: | I|\n",
      "Top 4th token. Logit: 14.37 Prob:  4.94% Token: | The|\n",
      "Top 5th token. Logit: 14.09 Prob:  3.75% Token: | If|\n",
      "Top 6th token. Logit: 14.07 Prob:  3.68% Token: | A|\n",
      "Top 7th token. Logit: 13.80 Prob:  2.79% Token: | You|\n",
      "Top 8th token. Logit: 13.26 Prob:  1.63% Token: | Please|\n",
      "Top 9th token. Logit: 12.99 Prob:  1.25% Token: | We|\n",
      "Top 10th token. Logit: 12.97 Prob:  1.22% Token: | In|\n",
      "Top 11th token. Logit: 12.87 Prob:  1.10% Token: | There|\n",
      "Top 12th token. Logit: 12.83 Prob:  1.06% Token: |\n",
      "\n",
      "|\n",
      "Top 13th token. Logit: 12.71 Prob:  0.94% Token: | For|\n",
      "Top 14th token. Logit: 12.66 Prob:  0.90% Token: | An|\n",
      "Top 15th token. Logit: 12.45 Prob:  0.73% Token: | See|\n",
      "Top 16th token. Logit: 12.38 Prob:  0.67% Token: |<|endoftext|>|\n",
      "Top 17th token. Logit: 12.26 Prob:  0.60% Token: | \"|\n",
      "Top 18th token. Logit: 12.24 Prob:  0.59% Token: | To|\n",
      "Top 19th token. Logit: 12.19 Prob:  0.56% Token: | As|\n",
      "Top 20th token. Logit: 12.13 Prob:  0.53% Token: | Read|\n",
      "Top 21th token. Logit: 12.02 Prob:  0.47% Token: | When|\n",
      "Top 22th token. Logit: 11.98 Prob:  0.46% Token: | Do|\n",
      "Top 23th token. Logit: 11.98 Prob:  0.45% Token: | That|\n",
      "Top 24th token. Logit: 11.94 Prob:  0.44% Token: | Let|\n",
      "Top 25th token. Logit: 11.93 Prob:  0.43% Token: | (|\n",
      "Top 26th token. Logit: 11.93 Prob:  0.43% Token: | No|\n",
      "Top 27th token. Logit: 11.86 Prob:  0.40% Token: | Your|\n",
      "Top 28th token. Logit: 11.82 Prob:  0.39% Token: | And|\n",
      "Top 29th token. Logit: 11.78 Prob:  0.37% Token: | What|\n",
      "Top 30th token. Logit: 11.75 Prob:  0.36% Token: | Don|\n",
      "Top 31th token. Logit: 11.67 Prob:  0.33% Token: | Here|\n",
      "Top 32th token. Logit: 11.53 Prob:  0.29% Token: | But|\n",
      "Top 33th token. Logit: 11.49 Prob:  0.28% Token: | All|\n",
      "Top 34th token. Logit: 11.47 Prob:  0.27% Token: | Be|\n",
      "Top 35th token. Logit: 11.46 Prob:  0.27% Token: | So|\n",
      "Top 36th token. Logit: 11.38 Prob:  0.25% Token: | Click|\n",
      "Top 37th token. Logit: 11.38 Prob:  0.25% Token: | Just|\n",
      "Top 38th token. Logit: 11.37 Prob:  0.25% Token: | Not|\n",
      "Top 39th token. Logit: 11.29 Prob:  0.23% Token: | Some|\n",
      "Top 40th token. Logit: 11.25 Prob:  0.22% Token: | Like|\n",
      "Top 41th token. Logit: 11.25 Prob:  0.22% Token: | One|\n",
      "Top 42th token. Logit: 11.23 Prob:  0.21% Token: | Go|\n",
      "Top 43th token. Logit: 11.21 Prob:  0.21% Token: | Each|\n",
      "Top 44th token. Logit: 11.20 Prob:  0.21% Token: | My|\n",
      "Top 45th token. Logit: 11.20 Prob:  0.21% Token: | First|\n",
      "Top 46th token. Logit: 11.14 Prob:  0.20% Token: | He|\n",
      "Top 47th token. Logit: 11.05 Prob:  0.18% Token: | Write|\n",
      "Top 48th token. Logit: 11.05 Prob:  0.18% Token: | Every|\n",
      "Top 49th token. Logit: 11.05 Prob:  0.18% Token: | Remember|\n",
      "Top 50th token. Logit: 11.02 Prob:  0.17% Token: | Take|\n",
      "Top 51th token. Logit: 11.02 Prob:  0.17% Token: | These|\n",
      "Top 52th token. Logit: 11.02 Prob:  0.17% Token: | Try|\n",
      "Top 53th token. Logit: 11.00 Prob:  0.17% Token: | Any|\n",
      "Top 54th token. Logit: 10.98 Prob:  0.17% Token: | While|\n",
      "Top 55th token. Logit: 10.98 Prob:  0.17% Token: | Its|\n",
      "Top 56th token. Logit: 10.98 Prob:  0.17% Token: | Because|\n",
      "Top 57th token. Logit: 10.97 Prob:  0.17% Token: | Think|\n",
      "Top 58th token. Logit: 10.89 Prob:  0.15% Token: | At|\n",
      "Top 59th token. Logit: 10.87 Prob:  0.15% Token: | After|\n",
      "Top 60th token. Logit: 10.79 Prob:  0.14% Token: | On|\n",
      "Top 61th token. Logit: 10.78 Prob:  0.14% Token: | Yes|\n",
      "Top 62th token. Logit: 10.75 Prob:  0.13% Token: | Get|\n",
      "Top 63th token. Logit: 10.75 Prob:  0.13% Token: | Feel|\n",
      "Top 64th token. Logit: 10.75 Prob:  0.13% Token: | Use|\n",
      "Top 65th token. Logit: 10.74 Prob:  0.13% Token: | By|\n",
      "Top 66th token. Logit: 10.73 Prob:  0.13% Token: | Make|\n",
      "Top 67th token. Logit: 10.72 Prob:  0.13% Token: | Or|\n",
      "Top 68th token. Logit: 10.71 Prob:  0.13% Token: | Although|\n",
      "Top 69th token. Logit: 10.71 Prob:  0.13% Token: | Have|\n",
      "Top 70th token. Logit: 10.69 Prob:  0.13% Token: | Find|\n",
      "Top 71th token. Logit: 10.69 Prob:  0.13% Token: | Once|\n",
      "Top 72th token. Logit: 10.69 Prob:  0.13% Token: | Words|\n",
      "Top 73th token. Logit: 10.66 Prob:  0.12% Token: | Nothing|\n",
      "Top 74th token. Logit: 10.64 Prob:  0.12% Token: | Sometimes|\n",
      "Top 75th token. Logit: 10.62 Prob:  0.12% Token: | Look|\n",
      "Top 76th token. Logit: 10.62 Prob:  0.12% Token: | Now|\n",
      "Top 77th token. Logit: 10.60 Prob:  0.11% Token: | From|\n",
      "Top 78th token. Logit: 10.57 Prob:  0.11% Token: | How|\n",
      "Top 79th token. Logit: 10.57 Prob:  0.11% Token: | [|\n",
      "Top 80th token. Logit: 10.55 Prob:  0.11% Token: | Learn|\n",
      "Top 81th token. Logit: 10.53 Prob:  0.11% Token: | Sorry|\n",
      "Top 82th token. Logit: 10.53 Prob:  0.11% Token: | Only|\n",
      "Top 83th token. Logit: 10.51 Prob:  0.10% Token: | Many|\n",
      "Top 84th token. Logit: 10.50 Prob:  0.10% Token: | Most|\n",
      "Top 85th token. Logit: 10.49 Prob:  0.10% Token: | Thank|\n",
      "Top 86th token. Logit: 10.44 Prob:  0.10% Token: | Note|\n",
      "Top 87th token. Logit: 10.41 Prob:  0.09% Token: | However|\n",
      "Top 88th token. Logit: 10.39 Prob:  0.09% Token: | Keep|\n",
      "Top 89th token. Logit: 10.39 Prob:  0.09% Token: | Follow|\n",
      "Top 90th token. Logit: 10.39 Prob:  0.09% Token: | Our|\n",
      "Top 91th token. Logit: 10.38 Prob:  0.09% Token: | Why|\n",
      "Top 92th token. Logit: 10.37 Prob:  0.09% Token: | Even|\n",
      "Top 93th token. Logit: 10.35 Prob:  0.09% Token: | Also|\n",
      "Top 94th token. Logit: 10.35 Prob:  0.09% Token: | Someone|\n",
      "Top 95th token. Logit: 10.32 Prob:  0.09% Token: | They|\n",
      "Top 96th token. Logit: 10.29 Prob:  0.08% Token: | Unless|\n",
      "Top 97th token. Logit: 10.27 Prob:  0.08% Token: | Is|\n",
      "Top 98th token. Logit: 10.25 Prob:  0.08% Token: | Something|\n",
      "Top 99th token. Logit: 10.23 Prob:  0.08% Token: | Perhaps|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' It'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' It'\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_prompt = \"This is a sentence.\"\n",
    "answer = \" It\"\n",
    "\n",
    "utils.test_prompt(example_prompt, answer, model, top_k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "- Interesting! The top result is a newline. Inspecting the rest of the results provides a nice reminder that tokens of the form `<space><capital-letter>` *are not only valid tokens to follow a full stop.* Other examples include:\n",
    "  - \"\\n\"\n",
    "  - \"\\n\\n\"\n",
    "  - `\"<|endoftext|>\"`\n",
    "  - Non-alphanumeric chars (preceded by spaces): ` (`, ` \"`\n",
    "\n",
    "- No numeric tokens are present. I expect logits for numerical tokens to increase for if the last character preceding the full stop was numeric. Quickly test below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'The', ' probability', ' of', ' me', ' getting', ' to', ' the', ' bottom', ' of', ' this', ' circuit', ' is', ' not', ' 0', '.']\n",
      "Tokenized answer: [' It']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.33</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.20</span><span style=\"font-weight: bold\">% Token: | It|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m9\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m15.33\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m2.20\u001b[0m\u001b[1m% Token: | It|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 16.66 Prob:  8.34% Token: |\n",
      "|\n",
      "Top 1th token. Logit: 16.62 Prob:  7.99% Token: |5|\n",
      "Top 2th token. Logit: 16.19 Prob:  5.20% Token: |1|\n",
      "Top 3th token. Logit: 15.69 Prob:  3.16% Token: |01|\n",
      "Top 4th token. Logit: 15.61 Prob:  2.91% Token: | The|\n",
      "Top 5th token. Logit: 15.59 Prob:  2.87% Token: |001|\n",
      "Top 6th token. Logit: 15.58 Prob:  2.84% Token: |0001|\n",
      "Top 7th token. Logit: 15.56 Prob:  2.77% Token: | If|\n",
      "Top 8th token. Logit: 15.55 Prob:  2.76% Token: |0|\n",
      "Top 9th token. Logit: 15.33 Prob:  2.20% Token: | It|\n",
      "Top 10th token. Logit: 15.13 Prob:  1.81% Token: |9|\n",
      "Top 11th token. Logit: 15.08 Prob:  1.72% Token: | But|\n",
      "Top 12th token. Logit: 14.95 Prob:  1.51% Token: |000|\n",
      "Top 13th token. Logit: 14.95 Prob:  1.51% Token: |6|\n",
      "Top 14th token. Logit: 14.94 Prob:  1.49% Token: |2|\n",
      "Top 15th token. Logit: 14.87 Prob:  1.39% Token: |25|\n",
      "Top 16th token. Logit: 14.81 Prob:  1.32% Token: |05|\n",
      "Top 17th token. Logit: 14.81 Prob:  1.32% Token: |8|\n",
      "Top 18th token. Logit: 14.81 Prob:  1.31% Token: | I|\n",
      "Top 19th token. Logit: 14.80 Prob:  1.30% Token: |00|\n",
      "Top 20th token. Logit: 14.72 Prob:  1.21% Token: |7|\n",
      "Top 21th token. Logit: 14.66 Prob:  1.13% Token: |0000|\n",
      "Top 22th token. Logit: 14.66 Prob:  1.13% Token: |3|\n",
      "Top 23th token. Logit: 14.53 Prob:  1.00% Token: | This|\n",
      "Top 24th token. Logit: 14.52 Prob:  0.98% Token: |75|\n",
      "Top 25th token. Logit: 14.51 Prob:  0.97% Token: | However|\n",
      "Top 26th token. Logit: 14.49 Prob:  0.95% Token: |4|\n",
      "Top 27th token. Logit: 14.42 Prob:  0.89% Token: | You|\n",
      "Top 28th token. Logit: 14.32 Prob:  0.80% Token: |99|\n",
      "Top 29th token. Logit: 14.26 Prob:  0.76% Token: | In|\n",
      "Top 30th token. Logit: 14.12 Prob:  0.66% Token: | That|\n",
      "Top 31th token. Logit: 14.07 Prob:  0.63% Token: |000000|\n",
      "Top 32th token. Logit: 13.95 Prob:  0.55% Token: | There|\n",
      "Top 33th token. Logit: 13.90 Prob:  0.53% Token: |9999|\n",
      "Top 34th token. Logit: 13.88 Prob:  0.52% Token: |02|\n",
      "Top 35th token. Logit: 13.88 Prob:  0.52% Token: |002|\n",
      "Top 36th token. Logit: 13.78 Prob:  0.47% Token: |00000000|\n",
      "Top 37th token. Logit: 13.75 Prob:  0.46% Token: |005|\n",
      "Top 38th token. Logit: 13.72 Prob:  0.44% Token: | So|\n",
      "Top 39th token. Logit: 13.59 Prob:  0.39% Token: |\n",
      "\n",
      "|\n",
      "Top 40th token. Logit: 13.56 Prob:  0.38% Token: |50|\n",
      "Top 41th token. Logit: 13.55 Prob:  0.37% Token: |10|\n",
      "Top 42th token. Logit: 13.48 Prob:  0.35% Token: | A|\n",
      "Top 43th token. Logit: 13.47 Prob:  0.35% Token: | For|\n",
      "Top 44th token. Logit: 13.42 Prob:  0.33% Token: |06|\n",
      "Top 45th token. Logit: 13.37 Prob:  0.31% Token: | We|\n",
      "Top 46th token. Logit: 13.34 Prob:  0.30% Token: |04|\n",
      "Top 47th token. Logit: 13.33 Prob:  0.30% Token: |999|\n",
      "Top 48th token. Logit: 13.28 Prob:  0.28% Token: |09|\n",
      "Top 49th token. Logit: 13.26 Prob:  0.28% Token: | Therefore|\n",
      "Top 50th token. Logit: 13.24 Prob:  0.27% Token: |<|endoftext|>|\n",
      "Top 51th token. Logit: 13.23 Prob:  0.27% Token: | As|\n",
      "Top 52th token. Logit: 13.21 Prob:  0.27% Token: |95|\n",
      "Top 53th token. Logit: 13.14 Prob:  0.25% Token: | (|\n",
      "Top 54th token. Logit: 13.14 Prob:  0.25% Token: | And|\n",
      "Top 55th token. Logit: 13.07 Prob:  0.23% Token: |07|\n",
      "Top 56th token. Logit: 13.05 Prob:  0.23% Token: |08|\n",
      "Top 57th token. Logit: 13.04 Prob:  0.22% Token: |03|\n",
      "Top 58th token. Logit: 13.02 Prob:  0.22% Token: |15|\n",
      "Top 59th token. Logit: 13.00 Prob:  0.22% Token: | One|\n",
      "Top 60th token. Logit: 12.95 Prob:  0.20% Token: |0000000000000000|\n",
      "Top 61th token. Logit: 12.94 Prob:  0.20% Token: |35|\n",
      "Top 62th token. Logit: 12.88 Prob:  0.19% Token: |33|\n",
      "Top 63th token. Logit: 12.87 Prob:  0.19% Token: |85|\n",
      "Top 64th token. Logit: 12.84 Prob:  0.18% Token: | When|\n",
      "Top 65th token. Logit: 12.83 Prob:  0.18% Token: |500|\n",
      "Top 66th token. Logit: 12.82 Prob:  0.18% Token: |0002|\n",
      "Top 67th token. Logit: 12.82 Prob:  0.18% Token: |618|\n",
      "Top 68th token. Logit: 12.82 Prob:  0.18% Token: |45|\n",
      "Top 69th token. Logit: 12.77 Prob:  0.17% Token: |025|\n",
      "Top 70th token. Logit: 12.76 Prob:  0.17% Token: | To|\n",
      "Top 71th token. Logit: 12.76 Prob:  0.17% Token: | What|\n",
      "Top 72th token. Logit: 12.75 Prob:  0.17% Token: | On|\n",
      "Top 73th token. Logit: 12.73 Prob:  0.16% Token: | 1|\n",
      "Top 74th token. Logit: 12.73 Prob:  0.16% Token: |00000|\n",
      "Top 75th token. Logit: 12.70 Prob:  0.16% Token: |97|\n",
      "Top 76th token. Logit: 12.69 Prob:  0.16% Token: |007|\n",
      "Top 77th token. Logit: 12.69 Prob:  0.16% Token: |67|\n",
      "Top 78th token. Logit: 12.60 Prob:  0.14% Token: | At|\n",
      "Top 79th token. Logit: 12.58 Prob:  0.14% Token: |12|\n",
      "Top 80th token. Logit: 12.53 Prob:  0.13% Token: |13|\n",
      "Top 81th token. Logit: 12.51 Prob:  0.13% Token: | Even|\n",
      "Top 82th token. Logit: 12.50 Prob:  0.13% Token: |006|\n",
      "Top 83th token. Logit: 12.48 Prob:  0.13% Token: | No|\n",
      "Top 84th token. Logit: 12.46 Prob:  0.12% Token: | Not|\n",
      "Top 85th token. Logit: 12.45 Prob:  0.12% Token: | .|\n",
      "Top 86th token. Logit: 12.44 Prob:  0.12% Token: | Then|\n",
      "Top 87th token. Logit: 12.44 Prob:  0.12% Token: |11|\n",
      "Top 88th token. Logit: 12.44 Prob:  0.12% Token: |49|\n",
      "Top 89th token. Logit: 12.41 Prob:  0.12% Token: |003|\n",
      "Top 90th token. Logit: 12.37 Prob:  0.11% Token: |37|\n",
      "Top 91th token. Logit: 12.37 Prob:  0.11% Token: | Because|\n",
      "Top 92th token. Logit: 12.34 Prob:  0.11% Token: | By|\n",
      "Top 93th token. Logit: 12.31 Prob:  0.11% Token: | Of|\n",
      "Top 94th token. Logit: 12.28 Prob:  0.10% Token: |20|\n",
      "Top 95th token. Logit: 12.28 Prob:  0.10% Token: |77|\n",
      "Top 96th token. Logit: 12.24 Prob:  0.10% Token: |27|\n",
      "Top 97th token. Logit: 12.23 Prob:  0.10% Token: |100|\n",
      "Top 98th token. Logit: 12.21 Prob:  0.10% Token: |87|\n",
      "Top 99th token. Logit: 12.21 Prob:  0.10% Token: |30|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' It'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' It'\u001b[0m, \u001b[1;36m9\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "numeric_example_prompt = (\n",
    "    \"The probability of me getting to the bottom of this circuit is not 0.\"\n",
    ")\n",
    "numeric_answer = \" It\"\n",
    "\n",
    "utils.test_prompt(numeric_example_prompt, numeric_answer, model, top_k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- Top token is `5`! No preceding space. (Upon reruns, sometimes it is second after `\"\\n\"`\")\n",
    "- Of the top 20 predictions, only 5 are non-numeric (one of which is a newline).\n",
    "\n",
    "- Will need to consider this in circuit exploration\n",
    "  - Possibly (likely?) a different circuit handles decimal points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple examples - stick with English prose for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Go.\",\n",
    "    \"Hello.\",\n",
    "    \"Matthew doesn't know what he is doing.\",\n",
    "    \"This is a sentence. This is another sentence.\",\n",
    "    \"The enigmatic, silver-haired professor, known for his eccentric lectures on quantum entanglement and the nature of reality, embarked on a perilous journey through the mist-shrouded mountains of Bhutan, seeking an ancient, mystical artifact rumored to hold the key to unlocking the secrets of the universe, while his loyal assistant, a quick-witted and resourceful graduate student with a penchant for solving cryptic puzzles, followed close behind, armed only with a weathered journal, a compass, and an unwavering determination to unravel the mystery that had haunted her mentor for decades.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model and get logits and cache for prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 116, 50257])\n",
      "torch.Size([5, 50257])\n",
      "Prompt length: 3\n",
      "Prompt as tokens: ['<|endoftext|>', 'Go', '.']\n",
      "Prompt length: 3\n",
      "Prompt as tokens: ['<|endoftext|>', 'Hello', '.']\n",
      "Prompt length: 10\n",
      "Prompt as tokens: ['<|endoftext|>', 'Matthew', ' doesn', \"'t\", ' know', ' what', ' he', ' is', ' doing', '.']\n",
      "Prompt length: 11\n",
      "Prompt as tokens: ['<|endoftext|>', 'This', ' is', ' a', ' sentence', '.', ' This', ' is', ' another', ' sentence', '.']\n",
      "Prompt length: 116\n",
      "Prompt as tokens: ['<|endoftext|>', 'The', ' enigmatic', ',', ' silver', '-', 'haired', ' professor', ',', ' known', ' for', ' his', ' eccentric', ' lectures', ' on', ' quantum', ' ent', 'ang', 'lement', ' and', ' the', ' nature', ' of', ' reality', ',', ' embarked', ' on', ' a', ' perilous', ' journey', ' through', ' the', ' mist', '-', 'sh', 'roud', 'ed', ' mountains', ' of', ' Bh', 'utan', ',', ' seeking', ' an', ' ancient', ',', ' mystical', ' artifact', ' rumored', ' to', ' hold', ' the', ' key', ' to', ' unlocking', ' the', ' secrets', ' of', ' the', ' universe', ',', ' while', ' his', ' loyal', ' assistant', ',', ' a', ' quick', '-', 'w', 'itted', ' and', ' resource', 'ful', ' graduate', ' student', ' with', ' a', ' penchant', ' for', ' solving', ' cryptic', ' puzzles', ',', ' followed', ' close', ' behind', ',', ' armed', ' only', ' with', ' a', ' we', 'athered', ' journal', ',', ' a', ' compass', ',', ' and', ' an', ' unw', 'avering', ' determination', ' to', ' unravel', ' the', ' mystery', ' that', ' had', ' haunted', ' her', ' mentor', ' for', ' decades', '.']\n",
      "\n",
      "Top prediction each prompt:\n",
      "['\\n', '\\n', '\\n', '\\n', ' But']\n"
     ]
    }
   ],
   "source": [
    "model.tokenizer.padding_side = \"left\"\n",
    "tokens = model.to_tokens(prompts)\n",
    "\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "print(logits.size())\n",
    "\n",
    "logits_final = logits[:, -1, :]\n",
    "print(logits_final.size())\n",
    "\n",
    "logits_sorted, logits_idx_sorted = logits_final.sort(\n",
    "    descending=True, stable=True, dim=-1\n",
    ")\n",
    "for prompt in prompts:\n",
    "    str_tokens = model.to_str_tokens(prompt)\n",
    "    print(\"Prompt length:\", len(str_tokens))\n",
    "    print(\"Prompt as tokens:\", str_tokens)\n",
    "print()\n",
    "print(f\"Top prediction each prompt:\\n{model.to_str_tokens(logits_idx_sorted[:, 1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top answers filtered by '\\<space\\>\\<titleword\\>' and corresponding incorrect answers\n",
    "\n",
    "- Could be missing something important here!\n",
    "\n",
    "- Implications for not taking very top? Hope it doesn't break later code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' Go', ' go', 'Go', 'go'), (' I', ' i', 'I', 'i'), (' He', ' he', 'He', 'he'), (' This', ' this', 'This', 'this'), (' But', ' but', 'But', 'but')]\n",
      "tensor([[1514,  467, 5247, 2188],\n",
      "        [ 314, 1312,   40,   72],\n",
      "        [ 679,  339, 1544,  258],\n",
      "        [ 770,  428, 1212, 5661],\n",
      "        [ 887,  475, 1537, 4360]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "space_titleword_pattern = re.compile(\"\\s[A-Z]\\w*\")\n",
    "\n",
    "def titleword_answer_generator(logits_idx_sorted):\n",
    "    \"\"\"Generate a sequence of correct/incorrect answer tuples by sampling\n",
    "    the first '<space><titleword>' token in `logits_sorted_idx` and\n",
    "    finding the corresponding incorrect versions. I.e.,\n",
    "    '<titleword>' (no space), '<space><lowerword>', and '<lowerword>'\n",
    "    \"\"\"\n",
    "    answer_str_tokens = []\n",
    "    answer_tokens = []\n",
    "\n",
    "    for logits_sorted_for_prompt in logits_idx_sorted:\n",
    "\n",
    "        predictions_sorted = model.to_str_tokens(logits_sorted_for_prompt)\n",
    "        top_space_titleword_prediction = next(\n",
    "            pred\n",
    "            for pred in predictions_sorted[:100]\n",
    "            if space_titleword_pattern.match(pred)\n",
    "        )\n",
    "        top_space_titleword_token = model.to_single_token(\n",
    "            top_space_titleword_prediction\n",
    "        )\n",
    "\n",
    "        lowercase_counterpart_str_token = top_space_titleword_prediction.lower()\n",
    "        lowercase_counterpart_token = model.to_single_token(\n",
    "            lowercase_counterpart_str_token\n",
    "        )\n",
    "\n",
    "        nospace_counterpart_str_token = top_space_titleword_prediction.lstrip()\n",
    "        nospace_counterpart_token = model.to_single_token(nospace_counterpart_str_token)\n",
    "\n",
    "        nospace_lowercase_counterpart_str_token = (\n",
    "            lowercase_counterpart_str_token.lstrip()\n",
    "        )\n",
    "        nospace_lowercase_counterpart_token = model.to_single_token(\n",
    "            nospace_lowercase_counterpart_str_token\n",
    "        )\n",
    "\n",
    "        answer_str_tokens.append(\n",
    "            (\n",
    "                top_space_titleword_prediction,\n",
    "                lowercase_counterpart_str_token,\n",
    "                nospace_counterpart_str_token,\n",
    "                nospace_lowercase_counterpart_str_token,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        answer_tokens.append(\n",
    "            (\n",
    "                top_space_titleword_token,\n",
    "                lowercase_counterpart_token,\n",
    "                nospace_counterpart_token,\n",
    "                nospace_lowercase_counterpart_token,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    answer_tokens = torch.tensor(answer_tokens).to(device)\n",
    "\n",
    "    return answer_tokens, answer_str_tokens\n",
    "\n",
    "\n",
    "answer_tokens, answer_str_tokens = titleword_answer_generator(logits_idx_sorted)\n",
    "print(answer_str_tokens)\n",
    "print(answer_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate logit diffs for each of the three incorrect answer types\n",
    "- Please forgive the weird formatting in the `logits_to_ave_logit_diff()` cell! I like to use the `black` formatter, but it's being annoyingly bugger for that particualr cell! I don't understand why and am mindful of the timesink that troubleshooting it likely presents..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_ave_logit_diff_2(\n",
    "    logits, answer_tokens, per_prompt=False, print_=True, incorrect_idx=1\n",
    "):\n",
    "    \"\"\"A modified version of `logits_to_ave_logit_diff_2()` from the Exploratory Analysis\n",
    "    Demo that permits multiple incorrect answers in `answer_tokens` where the user\n",
    "    can specify an incorrect answer index for which to calculate the logit diff.\n",
    "    \"\"\"\n",
    "    final_logits = logits[:, -1, :]\n",
    "\n",
    "    answer_logits = final_logits.gather(\n",
    "        dim=-1, index=answer_tokens[:, [0, incorrect_idx]]\n",
    "    )\n",
    "    answer_logit_diff = answer_logits[:, 0] - answer_logits[:, 1]\n",
    "    answer_logit_diff_mean = answer_logit_diff.mean()\n",
    "\n",
    "    if print_:\n",
    "        print(\n",
    "            \"Per prompt logit difference:\",\n",
    "            answer_logit_diff.detach().cpu().round(decimals=3),\n",
    "        )\n",
    "        print(\n",
    "            \"Average logit difference:\",\n",
    "            round(answer_logit_diff_mean.item(), 3),\n",
    "        )\n",
    "\n",
    "    if per_prompt:\n",
    "        return answer_logit_diff\n",
    "    else:\n",
    "        return answer_logit_diff_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts:\n",
      "'Go.'\n",
      "'Hello.'\n",
      "'Matthew doesn't know what he is doing.'\n",
      "'This is a sentence. This is another sentence.'\n",
      "'The enigmatic, silver-haired professor, known for his eccentric lectures on quantum entanglement and the nature of reality, embarked on a perilous journey through the mist-shrouded mountains of Bhutan, seeking an ancient, mystical artifact rumored to hold the key to unlocking the secrets of the universe, while his loyal assistant, a quick-witted and resourceful graduate student with a penchant for solving cryptic puzzles, followed close behind, armed only with a weathered journal, a compass, and an unwavering determination to unravel the mystery that had haunted her mentor for decades.'\n",
      "\n",
      "Top prediction per prompt:\n",
      "[' Go', ' I', ' He', ' This', ' But']\n",
      "\n",
      "Logit diffs for incorrect answer type: 'lowercase':\n",
      "Per prompt logit difference: tensor([5.0080, 5.9630, 7.7480, 6.4700, 7.8960])\n",
      "Average logit difference: 6.617\n",
      "\n",
      "Logit diffs for incorrect answer type: 'missing space':\n",
      "Per prompt logit difference: tensor([1.7910, 2.8110, 7.5470, 5.2680, 3.9150])\n",
      "Average logit difference: 4.266\n",
      "\n",
      "Logit diffs for incorrect answer type: 'lowercase and missing space':\n",
      "Per prompt logit difference: tensor([ 2.0660,  7.3190, 11.3110, 10.6940,  9.5120])\n",
      "Average logit difference: 8.181\n"
     ]
    }
   ],
   "source": [
    "idx_to_incorrect_answer_label = dict(\n",
    "    (\n",
    "        (1, \"lowercase\"),\n",
    "        (2, \"missing space\"),\n",
    "        (3, \"lowercase and missing space\"),\n",
    "    )\n",
    ")\n",
    "print(\"Prompts:\")\n",
    "for prompt in prompts:\n",
    "    print(f\"'{prompt}'\")\n",
    "\n",
    "print(f\"\\nTop prediction per prompt:\\n{[row[0] for row in answer_str_tokens]}\")\n",
    "for incorrect_idx in range(1, answer_tokens.size(1)):\n",
    "    print(\n",
    "        f\"\\nLogit diffs for incorrect answer type: '{idx_to_incorrect_answer_label[incorrect_idx]}':\"\n",
    "    )\n",
    "    logit_diffs = logits_to_ave_logit_diff_2(\n",
    "        logits, answer_tokens, per_prompt=True, incorrect_idx=incorrect_idx\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "- Average logit differences are high, with the min average implying $e^{4.266} \\approx 71 \\times$ more likely for the correct answer to be chosen then its incorrect counterpart\n",
    "\n",
    "- Missing space logit diff performs worst. Perhaps some of the training data included full-stop-separated words, like URLs?\n",
    "  - Although curiously missing space AND lowercase is least likely\n",
    "    - `\"go\"` following `\"Go.\"` is an outlier here\n",
    "  - Something is forcing the titlecasing?\n",
    "\n",
    "- Still very low data: cautious with above results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Logit Lens\n",
    "\n",
    "- Start with one incorrect case: lowercase version of same token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer residual directions shape: torch.Size([5, 2, 768])\n",
      "Logit difference directions shape: torch.Size([5, 768])\n"
     ]
    }
   ],
   "source": [
    "incorrect_idx = 1\n",
    "\n",
    "original_average_logit_diff = logits_to_ave_logit_diff_2(\n",
    "        logits, answer_tokens, per_prompt=False, incorrect_idx=incorrect_idx, print_=False)\n",
    "\n",
    "answer_residual_directions = model.tokens_to_residual_directions(answer_tokens[:, [0, incorrect_idx]])\n",
    "print(\"Answer residual directions shape:\", answer_residual_directions.shape)\n",
    "logit_diff_directions = (\n",
    "    answer_residual_directions[:, 0] - answer_residual_directions[:, 1]\n",
    ")\n",
    "print(\"Logit difference directions shape:\", logit_diff_directions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify okay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final residual stream shape: torch.Size([5, 116, 768])\n",
      "Calculated average logit diff: 7.233\n",
      "Original logit difference: 6.617\n"
     ]
    }
   ],
   "source": [
    "# cache syntax - resid_post is the residual stream at the end of the layer, -1 gets the final layer. The general syntax is [activation_name, layer_index, sub_layer_type].\n",
    "final_residual_stream = cache[\"resid_post\", -1]\n",
    "print(\"Final residual stream shape:\", final_residual_stream.shape)\n",
    "\n",
    "final_token_residual_stream = final_residual_stream[:, -1, :]\n",
    "# Apply LayerNorm scaling\n",
    "# pos_slice is the subset of the positions we take - here the final token of each prompt\n",
    "scaled_final_token_residual_stream = cache.apply_ln_to_stack(\n",
    "    final_token_residual_stream, layer=-1, pos_slice=-1\n",
    ")\n",
    "\n",
    "average_logit_diff = einsum(\n",
    "    \"batch d_model, batch d_model -> \",\n",
    "    scaled_final_token_residual_stream,\n",
    "    logit_diff_directions,\n",
    ") / len(prompts)\n",
    "print(\"Calculated average logit diff:\", round(average_logit_diff.item(), 3))\n",
    "print(\"Original logit difference:\", round(original_average_logit_diff.item(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **I don't understand the difference above. Let's simplify:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redefine `answers` and simpler `logit_to_ave_logit_diff()`. Stick to difference from lowercase version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1514,  467],\n",
      "        [ 314, 1312],\n",
      "        [ 679,  339],\n",
      "        [ 770,  428],\n",
      "        [ 887,  475]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "answers = [\n",
    "    (\" Go\", \" go\"),\n",
    "    (\" I\", \" i\"),\n",
    "    (\" He\", \" he\"),\n",
    "    (\" This\", \" this\"),\n",
    "    (\" But\", \" but\"),\n",
    "]\n",
    "\n",
    "answer_tokens = torch.tensor(\n",
    "    [[model.to_single_token(ans_row[0]), model.to_single_token(ans_row[1])] for ans_row in answers]\n",
    ").to(device)\n",
    "print(answer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original average logit diff: 6.617\n"
     ]
    }
   ],
   "source": [
    "def logits_to_ave_logit_diff(logits, answer_tokens, per_prompt=False):\n",
    "    # Only the final logits are relevant for the answer\n",
    "    final_logits = logits[:, -1, :]\n",
    "    answer_logits = final_logits.gather(dim=-1, index=answer_tokens)\n",
    "    answer_logit_diff = answer_logits[:, 0] - answer_logits[:, 1]\n",
    "    if per_prompt:\n",
    "        return answer_logit_diff\n",
    "    else:\n",
    "        return answer_logit_diff.mean()\n",
    "\n",
    "original_average_logit_diff = logits_to_ave_logit_diff(logits, answer_tokens, per_prompt=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that matches above..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Logit lens again\n",
    "\n",
    "- Code copied from [Exploratory Analysis Demo](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Exploratory_Analysis_Demo.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer residual directions shape: torch.Size([5, 2, 768])\n",
      "Logit difference directions shape: torch.Size([5, 768])\n",
      "Final residual stream shape: torch.Size([5, 116, 768])\n",
      "Calculated average logit diff: 7.233\n",
      "Original logit difference: 6.617\n"
     ]
    }
   ],
   "source": [
    "answer_residual_directions = model.tokens_to_residual_directions(answer_tokens)\n",
    "print(\"Answer residual directions shape:\", answer_residual_directions.shape)\n",
    "logit_diff_directions = (\n",
    "    answer_residual_directions[:, 0] - answer_residual_directions[:, 1]\n",
    ")\n",
    "print(\"Logit difference directions shape:\", logit_diff_directions.shape)\n",
    "\n",
    "# cache syntax - resid_post is the residual stream at the end of the layer, -1 gets the final layer. The general syntax is [activation_name, layer_index, sub_layer_type].\n",
    "final_residual_stream = cache[\"resid_post\", -1]\n",
    "print(\"Final residual stream shape:\", final_residual_stream.shape)\n",
    "final_token_residual_stream = final_residual_stream[:, -1, :]\n",
    "# Apply LayerNorm scaling\n",
    "# pos_slice is the subset of the positions we take - here the final token of each prompt\n",
    "scaled_final_token_residual_stream = cache.apply_ln_to_stack(\n",
    "    final_token_residual_stream, layer=-1, pos_slice=-1\n",
    ")\n",
    "\n",
    "average_logit_diff = einsum(\n",
    "    \"batch d_model, batch d_model -> \",\n",
    "    scaled_final_token_residual_stream,\n",
    "    logit_diff_directions,\n",
    ") / len(prompts)\n",
    "print(\"Calculated average logit diff:\", round(average_logit_diff.item(), 3))\n",
    "print(\"Original logit difference:\", round(original_average_logit_diff.item(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Still different! (Exactly the same numbers)\n",
    "\n",
    "- Though it at least appears that the the new logic in `logits_to_ave_logit_diff_2()` and the handling of more incorrect answers in `answer_tokens` is not the cause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try prompts of same length\n",
    "\n",
    "- Exploratory analysis demo warned about prompts of varying length\n",
    "\n",
    "- Perhaps the left padding flag is not enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt length: 11\n",
      "Prompt as tokens: ['<|endoftext|>', 'F', 'illing', ' up', ' to', ' eleven', ' tokens', '.', '\\n', 'Go', '.']\n",
      "Prompt length: 11\n",
      "Prompt as tokens: ['<|endoftext|>', 'Hello', '.', ' Hello', '.', ' Hello', '.', ' Hello', '.', ' Hello', '.']\n",
      "Prompt length: 11\n",
      "Prompt as tokens: ['<|endoftext|>', 'Matthew', ' really', ' does', ' not', ' know', ' what', ' he', ' is', ' doing', '.']\n",
      "Prompt length: 11\n",
      "Prompt as tokens: ['<|endoftext|>', 'This', ' is', ' a', ' sentence', '.', ' This', ' is', ' another', ' sentence', '.']\n",
      "\n",
      "Top prediction each prompt:\n",
      "[' Go', '\\n', '\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Filling up to eleven tokens.\\nGo.\",\n",
    "    \"Hello. Hello. Hello. Hello. Hello.\",\n",
    "    \"Matthew really does not know what he is doing.\",\n",
    "    \"This is a sentence. This is another sentence.\",\n",
    "]\n",
    "for prompt in prompts:\n",
    "    str_tokens = model.to_str_tokens(prompt)\n",
    "    print(\"Prompt length:\", len(str_tokens))\n",
    "    print(\"Prompt as tokens:\", str_tokens)\n",
    "\n",
    "tokens = model.to_tokens(prompts)\n",
    "\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "logits_final = logits[:, -1, :]\n",
    "logits_sorted, logits_idx_sorted = logits_final.sort(\n",
    "    descending=True, stable=True, dim=-1\n",
    ")\n",
    "print()\n",
    "print(f\"Top prediction each prompt:\\n{model.to_str_tokens(logits_idx_sorted[:, 1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate new answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' Go', ' go', 'Go', 'go'), (' Hello', ' hello', 'Hello', 'hello'), (' He', ' he', 'He', 'he'), (' This', ' this', 'This', 'this')]\n",
      "tensor([[ 1514,   467,  5247,  2188],\n",
      "        [18435, 23748, 15496, 31373],\n",
      "        [  679,   339,  1544,   258],\n",
      "        [  770,   428,  1212,  5661]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "answer_tokens, answer_str_tokens = titleword_answer_generator(logits_idx_sorted)\n",
    "print(answer_str_tokens)\n",
    "print(answer_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New logit diffs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts:\n",
      "'Filling up to eleven tokens.\n",
      "Go.'\n",
      "'Hello. Hello. Hello. Hello. Hello.'\n",
      "'Matthew really does not know what he is doing.'\n",
      "'This is a sentence. This is another sentence.'\n",
      "\n",
      "Top prediction per prompt:\n",
      "[' Go', ' Hello', ' He', ' This']\n",
      "\n",
      "Logit diffs for incorrect answer type: 'lowercase':\n",
      "Per prompt logit difference: tensor([5.5300, 5.4780, 6.6000, 6.4700])\n",
      "Average logit difference: 6.02\n",
      "\n",
      "Logit diffs for incorrect answer type: 'missing space':\n",
      "Per prompt logit difference: tensor([3.0500, 4.3010, 6.1450, 5.2680])\n",
      "Average logit difference: 4.691\n",
      "\n",
      "Logit diffs for incorrect answer type: 'lowercase and missing space':\n",
      "Per prompt logit difference: tensor([ 3.9800,  8.0950,  9.7580, 10.6940])\n",
      "Average logit difference: 8.132\n"
     ]
    }
   ],
   "source": [
    "idx_to_incorrect_answer_label = dict(\n",
    "    (\n",
    "        (1, \"lowercase\"),\n",
    "        (2, \"missing space\"),\n",
    "        (3, \"lowercase and missing space\"),\n",
    "    )\n",
    ")\n",
    "print(\"Prompts:\")\n",
    "for prompt in prompts:\n",
    "    print(f\"'{prompt}'\")\n",
    "\n",
    "print(f\"\\nTop prediction per prompt:\\n{[row[0] for row in answer_str_tokens]}\")\n",
    "for incorrect_idx in range(1, answer_tokens.size(1)):\n",
    "    print(\n",
    "        f\"\\nLogit diffs for incorrect answer type: '{idx_to_incorrect_answer_label[incorrect_idx]}':\"\n",
    "    )\n",
    "    logit_diffs = logits_to_ave_logit_diff_2(\n",
    "        logits, answer_tokens, per_prompt=True, incorrect_idx=incorrect_idx\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_idx = 2\n",
    "\n",
    "original_average_logit_diff = logits_to_ave_logit_diff_2(logits, answer_tokens, per_prompt=False, incorrect_idx=incorrect_idx, print_=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer residual directions shape: torch.Size([4, 4, 768])\n",
      "Logit difference directions shape: torch.Size([4, 768])\n",
      "Final residual stream shape: torch.Size([4, 11, 768])\n",
      "Calculated average logit diff: 6.871\n",
      "Original logit difference: 6.02\n"
     ]
    }
   ],
   "source": [
    "answer_residual_directions = model.tokens_to_residual_directions(answer_tokens)\n",
    "print(\"Answer residual directions shape:\", answer_residual_directions.shape)\n",
    "\n",
    "logit_diff_directions = (\n",
    "    answer_residual_directions[:, 0] - answer_residual_directions[:, incorrect_idx]\n",
    ")\n",
    "print(\"Logit difference directions shape:\", logit_diff_directions.shape)\n",
    "\n",
    "# cache syntax - resid_post is the residual stream at the end of the layer, -1 gets the final layer. The general syntax is [activation_name, layer_index, sub_layer_type].\n",
    "final_residual_stream = cache[\"resid_post\", -1]\n",
    "print(\"Final residual stream shape:\", final_residual_stream.shape)\n",
    "final_token_residual_stream = final_residual_stream[:, -1, :]\n",
    "# Apply LayerNorm scaling\n",
    "# pos_slice is the subset of the positions we take - here the final token of each prompt\n",
    "scaled_final_token_residual_stream = cache.apply_ln_to_stack(\n",
    "    final_token_residual_stream, layer=-1, pos_slice=-1\n",
    ")\n",
    "\n",
    "average_logit_diff = einsum(\n",
    "    \"batch d_model, batch d_model -> \",\n",
    "    scaled_final_token_residual_stream,\n",
    "    logit_diff_directions,\n",
    ") / len(prompts)\n",
    "print(\"Calculated average logit diff:\", round(average_logit_diff.item(), 3))\n",
    "print(\"Original logit difference:\", round(original_average_logit_diff.item(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Still different. In the interest of time, let's press on.\n",
    "\n",
    "- I think a possible cause is that I have not not been picking the top answer for each prompt\n",
    "as the correct answer.\n",
    "  - E.g., there may be some difference in scaling across the prompts that makes comparing\n",
    "an average invalid. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1514,  467],\n",
      "        [ 314, 1312],\n",
      "        [ 679,  339],\n",
      "        [ 770,  428],\n",
      "        [ 887,  475]], device='cuda:0')\n",
      "Answer residual directions shape: torch.Size([5, 2, 768])\n",
      "Logit difference directions shape: torch.Size([5, 768])\n",
      "Final residual stream shape: torch.Size([5, 2, 768])\n",
      "Calculated average logit diff: -3.219\n",
      "Original logit difference: -3.191\n"
     ]
    }
   ],
   "source": [
    "top_answers = [\n",
    "    (\" Go\", \" go\"),\n",
    "    (\" \\n\", \" i\"),\n",
    "    (\" \\n\", \" he\"),\n",
    "    (\" \\n\", \" this\"),\n",
    "    (\" \\n\", \" but\"),\n",
    "]\n",
    "\n",
    "top_answer_tokens = torch.tensor(\n",
    "    [[model.to_single_token(ans_row[0]), model.to_single_token(ans_row[1])] for ans_row in answers]\n",
    ").to(device)\n",
    "print(top_answer_tokens)\n",
    "logits_top_answer, cache_top_answer = model.run_with_cache(top_answer_tokens)\n",
    "original_average_logit_diff = logits_to_ave_logit_diff_2(logits_top_answer, top_answer_tokens, per_prompt=False, incorrect_idx=1, print_=False)\n",
    "\n",
    "answer_residual_directions = model.tokens_to_residual_directions(top_answer_tokens)\n",
    "print(\"Answer residual directions shape:\", answer_residual_directions.shape)\n",
    "\n",
    "logit_diff_directions = (\n",
    "    answer_residual_directions[:, 0] - answer_residual_directions[:, 1]\n",
    ")\n",
    "print(\"Logit difference directions shape:\", logit_diff_directions.shape)\n",
    "\n",
    "# cache syntax - resid_post is the residual stream at the end of the layer, -1 gets the final layer. The general syntax is [activation_name, layer_index, sub_layer_type].\n",
    "final_residual_stream = cache_top_answer[\"resid_post\", -1]\n",
    "print(\"Final residual stream shape:\", final_residual_stream.shape)\n",
    "final_token_residual_stream = final_residual_stream[:, -1, :]\n",
    "# Apply LayerNorm scaling\n",
    "# pos_slice is the subset of the positions we take - here the final token of each prompt\n",
    "scaled_final_token_residual_stream = cache_top_answer.apply_ln_to_stack(\n",
    "    final_token_residual_stream, layer=-1, pos_slice=-1\n",
    ")\n",
    "\n",
    "average_logit_diff = einsum(\n",
    "    \"batch d_model, batch d_model -> \",\n",
    "    scaled_final_token_residual_stream,\n",
    "    logit_diff_directions,\n",
    ") / len(prompts)\n",
    "print(\"Calculated average logit diff:\", round(average_logit_diff.item(), 3))\n",
    "print(\"Original logit difference:\", round(original_average_logit_diff.item(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well this is much closer! But still nto spot on as in Exploratory Analysis demo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
