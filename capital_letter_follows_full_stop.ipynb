{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does GPT-2 Small Predict Capital Letters After Full Stops? - *Exploratory Analysis*\n",
    "### Matthew Jennings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import einops\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import torch\n",
    "from circuitsvis.attention import attention_heads\n",
    "from fancy_einsum import einsum\n",
    "from IPython.display import HTML, IFrame\n",
    "from jaxtyping import Float\n",
    "\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens import ActivationCache, HookedTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x248983edc10>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define plotting helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor, **kwargs):\n",
    "    px.imshow(\n",
    "        utils.to_numpy(tensor),\n",
    "        color_continuous_midpoint=0.0,\n",
    "        color_continuous_scale=\"RdBu\",\n",
    "        **kwargs,\n",
    "    ).show()\n",
    "\n",
    "\n",
    "def line(tensor, **kwargs):\n",
    "    px.line(\n",
    "        y=utils.to_numpy(tensor),\n",
    "        **kwargs,\n",
    "    ).show()\n",
    "\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(\n",
    "        y=y,\n",
    "        x=x,\n",
    "        labels={\"x\": xaxis, \"y\": yaxis, \"color\": caxis},\n",
    "        **kwargs,\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    ")\n",
    "\n",
    "# Get the default device used\n",
    "device: torch.device = utils.get_device()\n",
    "print(f\"Pytorch device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple first example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the model correctly predict capital letters after a full stop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'This', ' is', ' a', ' sentence', '.']\n",
      "Tokenized answer: [' It']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.14</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.67</span><span style=\"font-weight: bold\">% Token: | It|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m15.14\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m10.67\u001b[0m\u001b[1m% Token: | It|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 15.64 Prob: 17.54% Token: |\n",
      "|\n",
      "Top 1th token. Logit: 15.14 Prob: 10.67% Token: | It|\n",
      "Top 2th token. Logit: 14.47 Prob:  5.44% Token: | This|\n",
      "Top 3th token. Logit: 14.39 Prob:  5.03% Token: | I|\n",
      "Top 4th token. Logit: 14.37 Prob:  4.94% Token: | The|\n",
      "Top 5th token. Logit: 14.09 Prob:  3.75% Token: | If|\n",
      "Top 6th token. Logit: 14.07 Prob:  3.68% Token: | A|\n",
      "Top 7th token. Logit: 13.80 Prob:  2.79% Token: | You|\n",
      "Top 8th token. Logit: 13.26 Prob:  1.63% Token: | Please|\n",
      "Top 9th token. Logit: 12.99 Prob:  1.25% Token: | We|\n",
      "Top 10th token. Logit: 12.97 Prob:  1.22% Token: | In|\n",
      "Top 11th token. Logit: 12.87 Prob:  1.10% Token: | There|\n",
      "Top 12th token. Logit: 12.83 Prob:  1.06% Token: |\n",
      "\n",
      "|\n",
      "Top 13th token. Logit: 12.71 Prob:  0.94% Token: | For|\n",
      "Top 14th token. Logit: 12.66 Prob:  0.90% Token: | An|\n",
      "Top 15th token. Logit: 12.45 Prob:  0.73% Token: | See|\n",
      "Top 16th token. Logit: 12.38 Prob:  0.67% Token: |<|endoftext|>|\n",
      "Top 17th token. Logit: 12.26 Prob:  0.60% Token: | \"|\n",
      "Top 18th token. Logit: 12.24 Prob:  0.59% Token: | To|\n",
      "Top 19th token. Logit: 12.19 Prob:  0.56% Token: | As|\n",
      "Top 20th token. Logit: 12.13 Prob:  0.53% Token: | Read|\n",
      "Top 21th token. Logit: 12.02 Prob:  0.47% Token: | When|\n",
      "Top 22th token. Logit: 11.98 Prob:  0.46% Token: | Do|\n",
      "Top 23th token. Logit: 11.98 Prob:  0.45% Token: | That|\n",
      "Top 24th token. Logit: 11.94 Prob:  0.44% Token: | Let|\n",
      "Top 25th token. Logit: 11.93 Prob:  0.43% Token: | (|\n",
      "Top 26th token. Logit: 11.93 Prob:  0.43% Token: | No|\n",
      "Top 27th token. Logit: 11.86 Prob:  0.40% Token: | Your|\n",
      "Top 28th token. Logit: 11.82 Prob:  0.39% Token: | And|\n",
      "Top 29th token. Logit: 11.78 Prob:  0.37% Token: | What|\n",
      "Top 30th token. Logit: 11.75 Prob:  0.36% Token: | Don|\n",
      "Top 31th token. Logit: 11.67 Prob:  0.33% Token: | Here|\n",
      "Top 32th token. Logit: 11.53 Prob:  0.29% Token: | But|\n",
      "Top 33th token. Logit: 11.49 Prob:  0.28% Token: | All|\n",
      "Top 34th token. Logit: 11.47 Prob:  0.27% Token: | Be|\n",
      "Top 35th token. Logit: 11.46 Prob:  0.27% Token: | So|\n",
      "Top 36th token. Logit: 11.38 Prob:  0.25% Token: | Click|\n",
      "Top 37th token. Logit: 11.38 Prob:  0.25% Token: | Just|\n",
      "Top 38th token. Logit: 11.37 Prob:  0.25% Token: | Not|\n",
      "Top 39th token. Logit: 11.29 Prob:  0.23% Token: | Some|\n",
      "Top 40th token. Logit: 11.25 Prob:  0.22% Token: | Like|\n",
      "Top 41th token. Logit: 11.25 Prob:  0.22% Token: | One|\n",
      "Top 42th token. Logit: 11.23 Prob:  0.21% Token: | Go|\n",
      "Top 43th token. Logit: 11.21 Prob:  0.21% Token: | Each|\n",
      "Top 44th token. Logit: 11.20 Prob:  0.21% Token: | My|\n",
      "Top 45th token. Logit: 11.20 Prob:  0.21% Token: | First|\n",
      "Top 46th token. Logit: 11.14 Prob:  0.20% Token: | He|\n",
      "Top 47th token. Logit: 11.05 Prob:  0.18% Token: | Write|\n",
      "Top 48th token. Logit: 11.05 Prob:  0.18% Token: | Every|\n",
      "Top 49th token. Logit: 11.05 Prob:  0.18% Token: | Remember|\n",
      "Top 50th token. Logit: 11.02 Prob:  0.17% Token: | Take|\n",
      "Top 51th token. Logit: 11.02 Prob:  0.17% Token: | These|\n",
      "Top 52th token. Logit: 11.02 Prob:  0.17% Token: | Try|\n",
      "Top 53th token. Logit: 11.00 Prob:  0.17% Token: | Any|\n",
      "Top 54th token. Logit: 10.98 Prob:  0.17% Token: | While|\n",
      "Top 55th token. Logit: 10.98 Prob:  0.17% Token: | Its|\n",
      "Top 56th token. Logit: 10.98 Prob:  0.17% Token: | Because|\n",
      "Top 57th token. Logit: 10.97 Prob:  0.17% Token: | Think|\n",
      "Top 58th token. Logit: 10.89 Prob:  0.15% Token: | At|\n",
      "Top 59th token. Logit: 10.87 Prob:  0.15% Token: | After|\n",
      "Top 60th token. Logit: 10.79 Prob:  0.14% Token: | On|\n",
      "Top 61th token. Logit: 10.78 Prob:  0.14% Token: | Yes|\n",
      "Top 62th token. Logit: 10.75 Prob:  0.13% Token: | Get|\n",
      "Top 63th token. Logit: 10.75 Prob:  0.13% Token: | Feel|\n",
      "Top 64th token. Logit: 10.75 Prob:  0.13% Token: | Use|\n",
      "Top 65th token. Logit: 10.74 Prob:  0.13% Token: | By|\n",
      "Top 66th token. Logit: 10.73 Prob:  0.13% Token: | Make|\n",
      "Top 67th token. Logit: 10.72 Prob:  0.13% Token: | Or|\n",
      "Top 68th token. Logit: 10.71 Prob:  0.13% Token: | Although|\n",
      "Top 69th token. Logit: 10.71 Prob:  0.13% Token: | Have|\n",
      "Top 70th token. Logit: 10.69 Prob:  0.13% Token: | Find|\n",
      "Top 71th token. Logit: 10.69 Prob:  0.13% Token: | Once|\n",
      "Top 72th token. Logit: 10.69 Prob:  0.13% Token: | Words|\n",
      "Top 73th token. Logit: 10.66 Prob:  0.12% Token: | Nothing|\n",
      "Top 74th token. Logit: 10.64 Prob:  0.12% Token: | Sometimes|\n",
      "Top 75th token. Logit: 10.62 Prob:  0.12% Token: | Look|\n",
      "Top 76th token. Logit: 10.62 Prob:  0.12% Token: | Now|\n",
      "Top 77th token. Logit: 10.60 Prob:  0.11% Token: | From|\n",
      "Top 78th token. Logit: 10.57 Prob:  0.11% Token: | How|\n",
      "Top 79th token. Logit: 10.57 Prob:  0.11% Token: | [|\n",
      "Top 80th token. Logit: 10.55 Prob:  0.11% Token: | Learn|\n",
      "Top 81th token. Logit: 10.53 Prob:  0.11% Token: | Sorry|\n",
      "Top 82th token. Logit: 10.53 Prob:  0.11% Token: | Only|\n",
      "Top 83th token. Logit: 10.51 Prob:  0.10% Token: | Many|\n",
      "Top 84th token. Logit: 10.50 Prob:  0.10% Token: | Most|\n",
      "Top 85th token. Logit: 10.49 Prob:  0.10% Token: | Thank|\n",
      "Top 86th token. Logit: 10.44 Prob:  0.10% Token: | Note|\n",
      "Top 87th token. Logit: 10.41 Prob:  0.09% Token: | However|\n",
      "Top 88th token. Logit: 10.39 Prob:  0.09% Token: | Keep|\n",
      "Top 89th token. Logit: 10.39 Prob:  0.09% Token: | Follow|\n",
      "Top 90th token. Logit: 10.39 Prob:  0.09% Token: | Our|\n",
      "Top 91th token. Logit: 10.38 Prob:  0.09% Token: | Why|\n",
      "Top 92th token. Logit: 10.37 Prob:  0.09% Token: | Even|\n",
      "Top 93th token. Logit: 10.35 Prob:  0.09% Token: | Also|\n",
      "Top 94th token. Logit: 10.35 Prob:  0.09% Token: | Someone|\n",
      "Top 95th token. Logit: 10.32 Prob:  0.09% Token: | They|\n",
      "Top 96th token. Logit: 10.29 Prob:  0.08% Token: | Unless|\n",
      "Top 97th token. Logit: 10.27 Prob:  0.08% Token: | Is|\n",
      "Top 98th token. Logit: 10.25 Prob:  0.08% Token: | Something|\n",
      "Top 99th token. Logit: 10.23 Prob:  0.08% Token: | Perhaps|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' It'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' It'\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_prompt = \"This is a sentence.\"\n",
    "answer = \" It\"\n",
    "\n",
    "utils.test_prompt(example_prompt, answer, model, top_k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "- Interesting! The top result is a newline. Inspecting the rest of the results provides a nice reminder that tokens of the form `<space><capital-letter>` *are not only valid tokens to follow a full stop.* Other examples include:\n",
    "  - \"\\n\"\n",
    "  - \"\\n\\n\"\n",
    "  - `\"<|endoftext|>\"`\n",
    "  - Non-alphanumeric chars (preceded by spaces): ` (`, ` \"`\n",
    "\n",
    "- No numeric tokens are present. I expect logits for numerical tokens to increase for if the last character preceding the full stop was numeric. Quickly test below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'The', ' probability', ' of', ' me', ' getting', ' to', ' the', ' bottom', ' of', ' this', ' circuit', ' is', ' not', ' 0', '.']\n",
      "Tokenized answer: [' It']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.33</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.20</span><span style=\"font-weight: bold\">% Token: | It|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m9\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m15.33\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m2.20\u001b[0m\u001b[1m% Token: | It|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 16.66 Prob:  8.34% Token: |\n",
      "|\n",
      "Top 1th token. Logit: 16.62 Prob:  7.99% Token: |5|\n",
      "Top 2th token. Logit: 16.19 Prob:  5.20% Token: |1|\n",
      "Top 3th token. Logit: 15.69 Prob:  3.16% Token: |01|\n",
      "Top 4th token. Logit: 15.61 Prob:  2.91% Token: | The|\n",
      "Top 5th token. Logit: 15.59 Prob:  2.87% Token: |001|\n",
      "Top 6th token. Logit: 15.58 Prob:  2.84% Token: |0001|\n",
      "Top 7th token. Logit: 15.56 Prob:  2.77% Token: | If|\n",
      "Top 8th token. Logit: 15.55 Prob:  2.76% Token: |0|\n",
      "Top 9th token. Logit: 15.33 Prob:  2.20% Token: | It|\n",
      "Top 10th token. Logit: 15.13 Prob:  1.81% Token: |9|\n",
      "Top 11th token. Logit: 15.08 Prob:  1.72% Token: | But|\n",
      "Top 12th token. Logit: 14.95 Prob:  1.51% Token: |000|\n",
      "Top 13th token. Logit: 14.95 Prob:  1.51% Token: |6|\n",
      "Top 14th token. Logit: 14.94 Prob:  1.49% Token: |2|\n",
      "Top 15th token. Logit: 14.87 Prob:  1.39% Token: |25|\n",
      "Top 16th token. Logit: 14.81 Prob:  1.32% Token: |05|\n",
      "Top 17th token. Logit: 14.81 Prob:  1.32% Token: |8|\n",
      "Top 18th token. Logit: 14.81 Prob:  1.31% Token: | I|\n",
      "Top 19th token. Logit: 14.80 Prob:  1.30% Token: |00|\n",
      "Top 20th token. Logit: 14.72 Prob:  1.21% Token: |7|\n",
      "Top 21th token. Logit: 14.66 Prob:  1.13% Token: |0000|\n",
      "Top 22th token. Logit: 14.66 Prob:  1.13% Token: |3|\n",
      "Top 23th token. Logit: 14.53 Prob:  1.00% Token: | This|\n",
      "Top 24th token. Logit: 14.52 Prob:  0.98% Token: |75|\n",
      "Top 25th token. Logit: 14.51 Prob:  0.97% Token: | However|\n",
      "Top 26th token. Logit: 14.49 Prob:  0.95% Token: |4|\n",
      "Top 27th token. Logit: 14.42 Prob:  0.89% Token: | You|\n",
      "Top 28th token. Logit: 14.32 Prob:  0.80% Token: |99|\n",
      "Top 29th token. Logit: 14.26 Prob:  0.76% Token: | In|\n",
      "Top 30th token. Logit: 14.12 Prob:  0.66% Token: | That|\n",
      "Top 31th token. Logit: 14.07 Prob:  0.63% Token: |000000|\n",
      "Top 32th token. Logit: 13.95 Prob:  0.55% Token: | There|\n",
      "Top 33th token. Logit: 13.90 Prob:  0.53% Token: |9999|\n",
      "Top 34th token. Logit: 13.88 Prob:  0.52% Token: |02|\n",
      "Top 35th token. Logit: 13.88 Prob:  0.52% Token: |002|\n",
      "Top 36th token. Logit: 13.78 Prob:  0.47% Token: |00000000|\n",
      "Top 37th token. Logit: 13.75 Prob:  0.46% Token: |005|\n",
      "Top 38th token. Logit: 13.72 Prob:  0.44% Token: | So|\n",
      "Top 39th token. Logit: 13.59 Prob:  0.39% Token: |\n",
      "\n",
      "|\n",
      "Top 40th token. Logit: 13.56 Prob:  0.38% Token: |50|\n",
      "Top 41th token. Logit: 13.55 Prob:  0.37% Token: |10|\n",
      "Top 42th token. Logit: 13.48 Prob:  0.35% Token: | A|\n",
      "Top 43th token. Logit: 13.47 Prob:  0.35% Token: | For|\n",
      "Top 44th token. Logit: 13.42 Prob:  0.33% Token: |06|\n",
      "Top 45th token. Logit: 13.37 Prob:  0.31% Token: | We|\n",
      "Top 46th token. Logit: 13.34 Prob:  0.30% Token: |04|\n",
      "Top 47th token. Logit: 13.33 Prob:  0.30% Token: |999|\n",
      "Top 48th token. Logit: 13.28 Prob:  0.28% Token: |09|\n",
      "Top 49th token. Logit: 13.26 Prob:  0.28% Token: | Therefore|\n",
      "Top 50th token. Logit: 13.24 Prob:  0.27% Token: |<|endoftext|>|\n",
      "Top 51th token. Logit: 13.23 Prob:  0.27% Token: | As|\n",
      "Top 52th token. Logit: 13.21 Prob:  0.27% Token: |95|\n",
      "Top 53th token. Logit: 13.14 Prob:  0.25% Token: | (|\n",
      "Top 54th token. Logit: 13.14 Prob:  0.25% Token: | And|\n",
      "Top 55th token. Logit: 13.07 Prob:  0.23% Token: |07|\n",
      "Top 56th token. Logit: 13.05 Prob:  0.23% Token: |08|\n",
      "Top 57th token. Logit: 13.04 Prob:  0.22% Token: |03|\n",
      "Top 58th token. Logit: 13.02 Prob:  0.22% Token: |15|\n",
      "Top 59th token. Logit: 13.00 Prob:  0.22% Token: | One|\n",
      "Top 60th token. Logit: 12.95 Prob:  0.20% Token: |0000000000000000|\n",
      "Top 61th token. Logit: 12.94 Prob:  0.20% Token: |35|\n",
      "Top 62th token. Logit: 12.88 Prob:  0.19% Token: |33|\n",
      "Top 63th token. Logit: 12.87 Prob:  0.19% Token: |85|\n",
      "Top 64th token. Logit: 12.84 Prob:  0.18% Token: | When|\n",
      "Top 65th token. Logit: 12.83 Prob:  0.18% Token: |500|\n",
      "Top 66th token. Logit: 12.82 Prob:  0.18% Token: |0002|\n",
      "Top 67th token. Logit: 12.82 Prob:  0.18% Token: |618|\n",
      "Top 68th token. Logit: 12.82 Prob:  0.18% Token: |45|\n",
      "Top 69th token. Logit: 12.77 Prob:  0.17% Token: |025|\n",
      "Top 70th token. Logit: 12.76 Prob:  0.17% Token: | To|\n",
      "Top 71th token. Logit: 12.76 Prob:  0.17% Token: | What|\n",
      "Top 72th token. Logit: 12.75 Prob:  0.17% Token: | On|\n",
      "Top 73th token. Logit: 12.73 Prob:  0.16% Token: | 1|\n",
      "Top 74th token. Logit: 12.73 Prob:  0.16% Token: |00000|\n",
      "Top 75th token. Logit: 12.70 Prob:  0.16% Token: |97|\n",
      "Top 76th token. Logit: 12.69 Prob:  0.16% Token: |007|\n",
      "Top 77th token. Logit: 12.69 Prob:  0.16% Token: |67|\n",
      "Top 78th token. Logit: 12.60 Prob:  0.14% Token: | At|\n",
      "Top 79th token. Logit: 12.58 Prob:  0.14% Token: |12|\n",
      "Top 80th token. Logit: 12.53 Prob:  0.13% Token: |13|\n",
      "Top 81th token. Logit: 12.51 Prob:  0.13% Token: | Even|\n",
      "Top 82th token. Logit: 12.50 Prob:  0.13% Token: |006|\n",
      "Top 83th token. Logit: 12.48 Prob:  0.13% Token: | No|\n",
      "Top 84th token. Logit: 12.46 Prob:  0.12% Token: | Not|\n",
      "Top 85th token. Logit: 12.45 Prob:  0.12% Token: | .|\n",
      "Top 86th token. Logit: 12.44 Prob:  0.12% Token: | Then|\n",
      "Top 87th token. Logit: 12.44 Prob:  0.12% Token: |11|\n",
      "Top 88th token. Logit: 12.44 Prob:  0.12% Token: |49|\n",
      "Top 89th token. Logit: 12.41 Prob:  0.12% Token: |003|\n",
      "Top 90th token. Logit: 12.37 Prob:  0.11% Token: |37|\n",
      "Top 91th token. Logit: 12.37 Prob:  0.11% Token: | Because|\n",
      "Top 92th token. Logit: 12.34 Prob:  0.11% Token: | By|\n",
      "Top 93th token. Logit: 12.31 Prob:  0.11% Token: | Of|\n",
      "Top 94th token. Logit: 12.28 Prob:  0.10% Token: |20|\n",
      "Top 95th token. Logit: 12.28 Prob:  0.10% Token: |77|\n",
      "Top 96th token. Logit: 12.24 Prob:  0.10% Token: |27|\n",
      "Top 97th token. Logit: 12.23 Prob:  0.10% Token: |100|\n",
      "Top 98th token. Logit: 12.21 Prob:  0.10% Token: |87|\n",
      "Top 99th token. Logit: 12.21 Prob:  0.10% Token: |30|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' It'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' It'\u001b[0m, \u001b[1;36m9\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "numeric_example_prompt = (\n",
    "    \"The probability of me getting to the bottom of this circuit is not 0.\"\n",
    ")\n",
    "numeric_answer = \" It\"\n",
    "\n",
    "utils.test_prompt(numeric_example_prompt, numeric_answer, model, top_k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- Top token is `5`! No preceding space. (Upon reruns, sometimes it is second after `\"\\n\"`\")\n",
    "- Of the top 20 predictions, only 5 are non-numeric (one of which is a newline).\n",
    "\n",
    "- Will need to consider this in circuit exploration\n",
    "  - Possibly (likely?) a different circuit handles decimal points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'this', ' is', ' an', ' unc', 'ap', 'ital', 'ised', ' sentence', '.']\n",
      "Tokenized answer: [' It']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.69</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.81</span><span style=\"font-weight: bold\">% Token: | It|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m10.69\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m2.81\u001b[0m\u001b[1m% Token: | It|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 12.34 Prob: 14.65% Token: |\n",
      "|\n",
      "Top 1th token. Logit: 11.46 Prob:  6.12% Token: | I|\n",
      "Top 2th token. Logit: 10.95 Prob:  3.66% Token: |\n",
      "\n",
      "|\n",
      "Top 3th token. Logit: 10.69 Prob:  2.81% Token: | It|\n",
      "Top 4th token. Logit: 10.68 Prob:  2.81% Token: | it|\n",
      "Top 5th token. Logit: 10.64 Prob:  2.69% Token: | the|\n",
      "Top 6th token. Logit: 10.49 Prob:  2.32% Token: | i|\n",
      "Top 7th token. Logit: 10.35 Prob:  2.01% Token: | The|\n",
      "Top 8th token. Logit: 10.18 Prob:  1.69% Token: | This|\n",
      "Top 9th token. Logit: 10.07 Prob:  1.52% Token: | if|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' It'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' It'\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "uncapitalised_example_prompt = (\n",
    "    \"this is an uncapitalised sentence.\"\n",
    ")\n",
    "uncapitalised_answer = \" It\"\n",
    "\n",
    "utils.test_prompt(uncapitalised_example_prompt, uncapitalised_answer, model, top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple examples - stick with English prose for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Go.\",\n",
    "    \"Hello.\",\n",
    "    \"Matthew doesn't know what he is doing.\",\n",
    "    \"This is a sentence. This is another sentence.\",\n",
    "    \"The enigmatic, silver-haired professor, known for his eccentric lectures on quantum entanglement and the nature of reality, embarked on a perilous journey through the mist-shrouded mountains of Bhutan, seeking an ancient, mystical artifact rumored to hold the key to unlocking the secrets of the universe, while his loyal assistant, a quick-witted and resourceful graduate student with a penchant for solving cryptic puzzles, followed close behind, armed only with a weathered journal, a compass, and an unwavering determination to unravel the mystery that had haunted her mentor for decades.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model and get logits and cache for prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 116, 50257])\n",
      "torch.Size([5, 50257])\n",
      "Prompt length: 3\n",
      "Prompt as tokens: ['<|endoftext|>', 'Go', '.']\n",
      "Prompt length: 3\n",
      "Prompt as tokens: ['<|endoftext|>', 'Hello', '.']\n",
      "Prompt length: 10\n",
      "Prompt as tokens: ['<|endoftext|>', 'Matthew', ' doesn', \"'t\", ' know', ' what', ' he', ' is', ' doing', '.']\n",
      "Prompt length: 11\n",
      "Prompt as tokens: ['<|endoftext|>', 'This', ' is', ' a', ' sentence', '.', ' This', ' is', ' another', ' sentence', '.']\n",
      "Prompt length: 116\n",
      "Prompt as tokens: ['<|endoftext|>', 'The', ' enigmatic', ',', ' silver', '-', 'haired', ' professor', ',', ' known', ' for', ' his', ' eccentric', ' lectures', ' on', ' quantum', ' ent', 'ang', 'lement', ' and', ' the', ' nature', ' of', ' reality', ',', ' embarked', ' on', ' a', ' perilous', ' journey', ' through', ' the', ' mist', '-', 'sh', 'roud', 'ed', ' mountains', ' of', ' Bh', 'utan', ',', ' seeking', ' an', ' ancient', ',', ' mystical', ' artifact', ' rumored', ' to', ' hold', ' the', ' key', ' to', ' unlocking', ' the', ' secrets', ' of', ' the', ' universe', ',', ' while', ' his', ' loyal', ' assistant', ',', ' a', ' quick', '-', 'w', 'itted', ' and', ' resource', 'ful', ' graduate', ' student', ' with', ' a', ' penchant', ' for', ' solving', ' cryptic', ' puzzles', ',', ' followed', ' close', ' behind', ',', ' armed', ' only', ' with', ' a', ' we', 'athered', ' journal', ',', ' a', ' compass', ',', ' and', ' an', ' unw', 'avering', ' determination', ' to', ' unravel', ' the', ' mystery', ' that', ' had', ' haunted', ' her', ' mentor', ' for', ' decades', '.']\n",
      "\n",
      "Top prediction each prompt:\n",
      "['\\n', '\\n', '\\n', '\\n', ' But']\n"
     ]
    }
   ],
   "source": [
    "model.tokenizer.padding_side = \"left\"\n",
    "tokens = model.to_tokens(prompts)\n",
    "\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "print(logits.size())\n",
    "\n",
    "logits_final = logits[:, -1, :]\n",
    "print(logits_final.size())\n",
    "\n",
    "logits_sorted, logits_idx_sorted = logits_final.sort(\n",
    "    descending=True, stable=True, dim=-1\n",
    ")\n",
    "for prompt in prompts:\n",
    "    str_tokens = model.to_str_tokens(prompt)\n",
    "    print(\"Prompt length:\", len(str_tokens))\n",
    "    print(\"Prompt as tokens:\", str_tokens)\n",
    "print()\n",
    "print(f\"Top prediction each prompt:\\n{model.to_str_tokens(logits_idx_sorted[:, 1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top answers filtered by '\\<space\\>\\<titleword\\>' and corresponding incorrect answers\n",
    "\n",
    "- Could be missing something important here!\n",
    "\n",
    "- Implications for not taking very top? Hope it doesn't break later code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' Go', ' go', 'Go', 'go'), (' I', ' i', 'I', 'i'), (' He', ' he', 'He', 'he'), (' This', ' this', 'This', 'this'), (' But', ' but', 'But', 'but')]\n",
      "tensor([[1514,  467, 5247, 2188],\n",
      "        [ 314, 1312,   40,   72],\n",
      "        [ 679,  339, 1544,  258],\n",
      "        [ 770,  428, 1212, 5661],\n",
      "        [ 887,  475, 1537, 4360]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "space_titleword_pattern = re.compile(\"\\s[A-Z]\\w*\")\n",
    "\n",
    "def titleword_answer_generator(logits_idx_sorted):\n",
    "    \"\"\"Generate a sequence of correct/incorrect answer tuples by sampling\n",
    "    the first '<space><titleword>' token in `logits_sorted_idx` and\n",
    "    finding the corresponding incorrect versions. I.e.,\n",
    "    '<titleword>' (no space), '<space><lowerword>', and '<lowerword>'\n",
    "    \"\"\"\n",
    "    answer_str_tokens = []\n",
    "    answer_tokens = []\n",
    "\n",
    "    for logits_sorted_for_prompt in logits_idx_sorted:\n",
    "\n",
    "        predictions_sorted = model.to_str_tokens(logits_sorted_for_prompt)\n",
    "        top_space_titleword_prediction = next(\n",
    "            pred\n",
    "            for pred in predictions_sorted[:100]\n",
    "            if space_titleword_pattern.match(pred)\n",
    "        )\n",
    "        top_space_titleword_token = model.to_single_token(\n",
    "            top_space_titleword_prediction\n",
    "        )\n",
    "\n",
    "        lowercase_counterpart_str_token = top_space_titleword_prediction.lower()\n",
    "        lowercase_counterpart_token = model.to_single_token(\n",
    "            lowercase_counterpart_str_token\n",
    "        )\n",
    "\n",
    "        nospace_counterpart_str_token = top_space_titleword_prediction.lstrip()\n",
    "        nospace_counterpart_token = model.to_single_token(nospace_counterpart_str_token)\n",
    "\n",
    "        nospace_lowercase_counterpart_str_token = (\n",
    "            lowercase_counterpart_str_token.lstrip()\n",
    "        )\n",
    "        nospace_lowercase_counterpart_token = model.to_single_token(\n",
    "            nospace_lowercase_counterpart_str_token\n",
    "        )\n",
    "\n",
    "        answer_str_tokens.append(\n",
    "            (\n",
    "                top_space_titleword_prediction,\n",
    "                lowercase_counterpart_str_token,\n",
    "                nospace_counterpart_str_token,\n",
    "                nospace_lowercase_counterpart_str_token,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        answer_tokens.append(\n",
    "            (\n",
    "                top_space_titleword_token,\n",
    "                lowercase_counterpart_token,\n",
    "                nospace_counterpart_token,\n",
    "                nospace_lowercase_counterpart_token,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    answer_tokens = torch.tensor(answer_tokens).to(device)\n",
    "\n",
    "    return answer_tokens, answer_str_tokens\n",
    "\n",
    "\n",
    "answer_tokens, answer_str_tokens = titleword_answer_generator(logits_idx_sorted)\n",
    "print(answer_str_tokens)\n",
    "print(answer_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate logit diffs for each of the three incorrect answer types\n",
    "- Please forgive the weird formatting in the `logits_to_ave_logit_diff()` cell! I like to use the `black` formatter, but it's being annoyingly bugger for that particualr cell! I don't understand why and am mindful of the timesink that troubleshooting it likely presents..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_ave_logit_diff_2(\n",
    "    logits, answer_tokens, per_prompt=False, print_=True, incorrect_idx=1\n",
    "):\n",
    "    \"\"\"A modified version of `logits_to_ave_logit_diff_2()` from the Exploratory Analysis\n",
    "    Demo that permits multiple incorrect answers in `answer_tokens` where the user\n",
    "    can specify an incorrect answer index for which to calculate the logit diff.\n",
    "    \"\"\"\n",
    "    final_logits = logits[:, -1, :]\n",
    "\n",
    "    answer_logits = final_logits.gather(\n",
    "        dim=-1, index=answer_tokens[:, [0, incorrect_idx]]\n",
    "    )\n",
    "    answer_logit_diff = answer_logits[:, 0] - answer_logits[:, 1]\n",
    "    answer_logit_diff_mean = answer_logit_diff.mean()\n",
    "\n",
    "    if print_:\n",
    "        print(\n",
    "            \"Per prompt logit difference:\",\n",
    "            answer_logit_diff.detach().cpu().round(decimals=3),\n",
    "        )\n",
    "        print(\n",
    "            \"Average logit difference:\",\n",
    "            round(answer_logit_diff_mean.item(), 3),\n",
    "        )\n",
    "\n",
    "    if per_prompt:\n",
    "        return answer_logit_diff\n",
    "    else:\n",
    "        return answer_logit_diff_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts:\n",
      "'Go.'\n",
      "'Hello.'\n",
      "'Matthew doesn't know what he is doing.'\n",
      "'This is a sentence. This is another sentence.'\n",
      "'The enigmatic, silver-haired professor, known for his eccentric lectures on quantum entanglement and the nature of reality, embarked on a perilous journey through the mist-shrouded mountains of Bhutan, seeking an ancient, mystical artifact rumored to hold the key to unlocking the secrets of the universe, while his loyal assistant, a quick-witted and resourceful graduate student with a penchant for solving cryptic puzzles, followed close behind, armed only with a weathered journal, a compass, and an unwavering determination to unravel the mystery that had haunted her mentor for decades.'\n",
      "\n",
      "Top prediction per prompt:\n",
      "[' Go', ' I', ' He', ' This', ' But']\n",
      "\n",
      "Logit diffs for incorrect answer type: 'lowercase':\n",
      "Per prompt logit difference: tensor([5.0080, 5.9630, 7.7480, 6.4700, 7.8960])\n",
      "Average logit difference: 6.617\n",
      "\n",
      "Logit diffs for incorrect answer type: 'missing space':\n",
      "Per prompt logit difference: tensor([1.7910, 2.8110, 7.5470, 5.2680, 3.9150])\n",
      "Average logit difference: 4.266\n",
      "\n",
      "Logit diffs for incorrect answer type: 'lowercase and missing space':\n",
      "Per prompt logit difference: tensor([ 2.0660,  7.3190, 11.3110, 10.6940,  9.5120])\n",
      "Average logit difference: 8.181\n"
     ]
    }
   ],
   "source": [
    "idx_to_incorrect_answer_label = dict(\n",
    "    (\n",
    "        (1, \"lowercase\"),\n",
    "        (2, \"missing space\"),\n",
    "        (3, \"lowercase and missing space\"),\n",
    "    )\n",
    ")\n",
    "print(\"Prompts:\")\n",
    "for prompt in prompts:\n",
    "    print(f\"'{prompt}'\")\n",
    "\n",
    "print(f\"\\nTop prediction per prompt:\\n{[row[0] for row in answer_str_tokens]}\")\n",
    "for incorrect_idx in range(1, answer_tokens.size(1)):\n",
    "    print(\n",
    "        f\"\\nLogit diffs for incorrect answer type: '{idx_to_incorrect_answer_label[incorrect_idx]}':\"\n",
    "    )\n",
    "    logit_diffs = logits_to_ave_logit_diff_2(\n",
    "        logits, answer_tokens, per_prompt=True, incorrect_idx=incorrect_idx\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "- Average logit differences are high, with the min average implying $e^{4.266} \\approx 71 \\times$ more likely for the correct answer to be chosen then its incorrect counterpart\n",
    "\n",
    "- Missing space logit diff performs worst. Perhaps some of the training data included full-stop-separated words, like URLs?\n",
    "  - Although curiously missing space AND lowercase is least likely\n",
    "    - `\"go\"` following `\"Go.\"` is an outlier here\n",
    "  - Something is forcing the titlecasing?\n",
    "\n",
    "- Still very low data: cautious with above results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Logit Lens\n",
    "\n",
    "- Start with one incorrect case: lowercase version of same token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer residual directions shape: torch.Size([5, 2, 768])\n",
      "Logit difference directions shape: torch.Size([5, 768])\n"
     ]
    }
   ],
   "source": [
    "incorrect_idx = 1\n",
    "\n",
    "original_average_logit_diff = logits_to_ave_logit_diff_2(\n",
    "        logits, answer_tokens, per_prompt=False, incorrect_idx=incorrect_idx, print_=False)\n",
    "\n",
    "answer_residual_directions = model.tokens_to_residual_directions(answer_tokens[:, [0, incorrect_idx]])\n",
    "print(\"Answer residual directions shape:\", answer_residual_directions.shape)\n",
    "logit_diff_directions = (\n",
    "    answer_residual_directions[:, 0] - answer_residual_directions[:, 1]\n",
    ")\n",
    "print(\"Logit difference directions shape:\", logit_diff_directions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify okay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final residual stream shape: torch.Size([5, 116, 768])\n",
      "Calculated average logit diff: 7.233\n",
      "Original logit difference: 6.617\n"
     ]
    }
   ],
   "source": [
    "# cache syntax - resid_post is the residual stream at the end of the layer, -1 gets the final layer. The general syntax is [activation_name, layer_index, sub_layer_type].\n",
    "final_residual_stream = cache[\"resid_post\", -1]\n",
    "print(\"Final residual stream shape:\", final_residual_stream.shape)\n",
    "\n",
    "final_token_residual_stream = final_residual_stream[:, -1, :]\n",
    "# Apply LayerNorm scaling\n",
    "# pos_slice is the subset of the positions we take - here the final token of each prompt\n",
    "scaled_final_token_residual_stream = cache.apply_ln_to_stack(\n",
    "    final_token_residual_stream, layer=-1, pos_slice=-1\n",
    ")\n",
    "\n",
    "average_logit_diff = einsum(\n",
    "    \"batch d_model, batch d_model -> \",\n",
    "    scaled_final_token_residual_stream,\n",
    "    logit_diff_directions,\n",
    ") / len(prompts)\n",
    "print(\"Calculated average logit diff:\", round(average_logit_diff.item(), 3))\n",
    "print(\"Original logit difference:\", round(original_average_logit_diff.item(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **I don't understand the difference above. Let's simplify:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redefine `answers` and simpler `logit_to_ave_logit_diff()`. Stick to difference from lowercase version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1514,  467],\n",
      "        [ 314, 1312],\n",
      "        [ 679,  339],\n",
      "        [ 770,  428],\n",
      "        [ 887,  475]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "answers = [\n",
    "    (\" Go\", \" go\"),\n",
    "    (\" I\", \" i\"),\n",
    "    (\" He\", \" he\"),\n",
    "    (\" This\", \" this\"),\n",
    "    (\" But\", \" but\"),\n",
    "]\n",
    "\n",
    "answer_tokens = torch.tensor(\n",
    "    [[model.to_single_token(ans_row[0]), model.to_single_token(ans_row[1])] for ans_row in answers]\n",
    ").to(device)\n",
    "print(answer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_ave_logit_diff(logits, answer_tokens, per_prompt=False):\n",
    "    # Only the final logits are relevant for the answer\n",
    "    final_logits = logits[:, -1, :]\n",
    "    answer_logits = final_logits.gather(dim=-1, index=answer_tokens)\n",
    "    answer_logit_diff = answer_logits[:, 0] - answer_logits[:, 1]\n",
    "    if per_prompt:\n",
    "        return answer_logit_diff\n",
    "    else:\n",
    "        return answer_logit_diff.mean()\n",
    "\n",
    "original_average_logit_diff = logits_to_ave_logit_diff(logits, answer_tokens, per_prompt=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that matches above..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Logit lens again\n",
    "\n",
    "- Code copied from [Exploratory Analysis Demo](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Exploratory_Analysis_Demo.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer residual directions shape: torch.Size([5, 2, 768])\n",
      "Logit difference directions shape: torch.Size([5, 768])\n",
      "Final residual stream shape: torch.Size([5, 116, 768])\n",
      "Calculated average logit diff: 7.233\n",
      "Original logit difference: 6.617\n"
     ]
    }
   ],
   "source": [
    "answer_residual_directions = model.tokens_to_residual_directions(answer_tokens)\n",
    "print(\"Answer residual directions shape:\", answer_residual_directions.shape)\n",
    "logit_diff_directions = (\n",
    "    answer_residual_directions[:, 0] - answer_residual_directions[:, 1]\n",
    ")\n",
    "print(\"Logit difference directions shape:\", logit_diff_directions.shape)\n",
    "\n",
    "# cache syntax - resid_post is the residual stream at the end of the layer, -1 gets the final layer. The general syntax is [activation_name, layer_index, sub_layer_type].\n",
    "final_residual_stream = cache[\"resid_post\", -1]\n",
    "print(\"Final residual stream shape:\", final_residual_stream.shape)\n",
    "final_token_residual_stream = final_residual_stream[:, -1, :]\n",
    "# Apply LayerNorm scaling\n",
    "# pos_slice is the subset of the positions we take - here the final token of each prompt\n",
    "scaled_final_token_residual_stream = cache.apply_ln_to_stack(\n",
    "    final_token_residual_stream, layer=-1, pos_slice=-1\n",
    ")\n",
    "\n",
    "average_logit_diff = einsum(\n",
    "    \"batch d_model, batch d_model -> \",\n",
    "    scaled_final_token_residual_stream,\n",
    "    logit_diff_directions,\n",
    ") / len(prompts)\n",
    "print(\"Calculated average logit diff:\", round(average_logit_diff.item(), 3))\n",
    "print(\"Original logit difference:\", round(original_average_logit_diff.item(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Still different! (Exactly the same numbers)\n",
    "\n",
    "- Though it at least appears that the the new logic in `logits_to_ave_logit_diff_2()` and the handling of more incorrect answers in `answer_tokens` is not the cause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try prompts of same length\n",
    "\n",
    "- Exploratory analysis demo warned about prompts of varying length\n",
    "\n",
    "- Perhaps the left padding flag is not enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt length: 11\n",
      "Prompt as tokens: ['<|endoftext|>', 'F', 'illing', ' up', ' to', ' eleven', ' tokens', '.', '\\n', 'Go', '.']\n",
      "Prompt length: 11\n",
      "Prompt as tokens: ['<|endoftext|>', 'Hello', '.', ' Hello', '.', ' Hello', '.', ' Hello', '.', ' Hello', '.']\n",
      "Prompt length: 11\n",
      "Prompt as tokens: ['<|endoftext|>', 'Yeah', ' Matt', ' doesn', \"'t\", ' know', ' what', ' he', ' is', ' doing', '.']\n",
      "Prompt length: 11\n",
      "Prompt as tokens: ['<|endoftext|>', 'That', ' Will', ' does', ' not', ' know', ' where', ' he', ' is', ' going', '.']\n",
      "Prompt length: 11\n",
      "Prompt as tokens: ['<|endoftext|>', 'Someone', ' should', ' really', ' help', ' them', ' out', ',', ' I', ' think', '.']\n",
      "Prompt length: 11\n",
      "Prompt as tokens: ['<|endoftext|>', 'It', ' is', ' wonderful', ' to', ' be', ' in', ' Adelaide', ' in', ' March', '.']\n",
      "Prompt length: 11\n",
      "Prompt as tokens: ['<|endoftext|>', 'Ko', 'al', 'as', ' are', ' cute', ',', ' but', ' gr', 'umpy', '.']\n",
      "Prompt length: 11\n",
      "Prompt as tokens: ['<|endoftext|>', 'This', ' is', ' a', ' sentence', '.', ' This', ' is', ' another', ' sentence', '.']\n",
      "\n",
      "Top prediction each prompt:\n",
      "[' Go', '\\n', '\\n', ' He', ' I', ' It', '\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Filling up to eleven tokens.\\nGo.\", # Single word sentence\n",
    "    \"Hello. Hello. Hello. Hello. Hello.\", # Repeat single word sentence\n",
    "    \"Yeah Matt doesn't know what he is doing.\", # More or less standard sentence\n",
    "    \"That Will does not know where he is going.\", # More or less standard sentence, different end verb\n",
    "    \"Someone should really help them out, I think.\", # More or less standard sentence, not \"ing\" end verb, has comma\n",
    "    \"It is wonderful to be in Adelaide in March.\", # More or less standard sentence, end noun, has comma\n",
    "    \"Koalas are cute, but grumpy.\", # More or less standard sentence, end in adjective\n",
    "    \"This is a sentence. This is another sentence.\", # Two sentences\n",
    "]\n",
    "for prompt in prompts:\n",
    "    str_tokens = model.to_str_tokens(prompt)\n",
    "    print(\"Prompt length:\", len(str_tokens))\n",
    "    print(\"Prompt as tokens:\", str_tokens)\n",
    "\n",
    "tokens = model.to_tokens(prompts)\n",
    "\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "logits_final = logits[:, -1, :]\n",
    "logits_sorted, logits_idx_sorted = logits_final.sort(\n",
    "    descending=True, stable=True, dim=-1\n",
    ")\n",
    "print()\n",
    "print(f\"Top prediction each prompt:\\n{model.to_str_tokens(logits_idx_sorted[:, 1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate new answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' Go', ' go', 'Go', 'go'), (' Hello', ' hello', 'Hello', 'hello'), (' He', ' he', 'He', 'he'), (' He', ' he', 'He', 'he'), (' I', ' i', 'I', 'i'), (' It', ' it', 'It', 'it'), (' They', ' they', 'They', 'they'), (' This', ' this', 'This', 'this')]\n",
      "tensor([[ 1514,   467,  5247,  2188],\n",
      "        [18435, 23748, 15496, 31373],\n",
      "        [  679,   339,  1544,   258],\n",
      "        [  679,   339,  1544,   258],\n",
      "        [  314,  1312,    40,    72],\n",
      "        [  632,   340,  1026,   270],\n",
      "        [ 1119,   484,  2990,  9930],\n",
      "        [  770,   428,  1212,  5661]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "answer_tokens, answer_str_tokens = titleword_answer_generator(logits_idx_sorted)\n",
    "print(answer_str_tokens)\n",
    "print(answer_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New logit diffs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts:\n",
      "'Filling up to eleven tokens.\n",
      "Go.'\n",
      "'Hello. Hello. Hello. Hello. Hello.'\n",
      "'Yeah Matt doesn't know what he is doing.'\n",
      "'That Will does not know where he is going.'\n",
      "'Someone should really help them out, I think.'\n",
      "'It is wonderful to be in Adelaide in March.'\n",
      "'Koalas are cute, but grumpy.'\n",
      "'This is a sentence. This is another sentence.'\n",
      "\n",
      "Top space-titleword prediction per prompt:\n",
      "[' Go', ' Hello', ' He', ' He', ' I', ' It', ' They', ' This']\n",
      "\n",
      "Logit diffs for incorrect answer type: 'lowercase':\n",
      "Per prompt logit difference: tensor([5.5300, 5.4780, 5.8090, 6.1450, 6.2310, 7.7480, 8.2100, 6.4700])\n",
      "Average logit difference: 6.453\n",
      "\n",
      "Logit diffs for incorrect answer type: 'missing space':\n",
      "Per prompt logit difference: tensor([3.0500, 4.3010, 5.8150, 5.5950, 3.9660, 6.5970, 6.3920, 5.2680])\n",
      "Average logit difference: 5.123\n",
      "\n",
      "Logit diffs for incorrect answer type: 'lowercase and missing space':\n",
      "Per prompt logit difference: tensor([ 3.9800,  8.0950,  9.1950,  9.6280, 10.1990, 11.4920, 11.1240, 10.6940])\n",
      "Average logit difference: 9.301\n"
     ]
    }
   ],
   "source": [
    "idx_to_incorrect_answer_label = dict(\n",
    "    (\n",
    "        (1, \"lowercase\"),\n",
    "        (2, \"missing space\"),\n",
    "        (3, \"lowercase and missing space\"),\n",
    "    )\n",
    ")\n",
    "print(\"Prompts:\")\n",
    "for prompt in prompts:\n",
    "    print(f\"'{prompt}'\")\n",
    "\n",
    "print(f\"\\nTop space-titleword prediction per prompt:\\n{[row[0] for row in answer_str_tokens]}\")\n",
    "for incorrect_idx in range(1, answer_tokens.size(1)):\n",
    "    print(\n",
    "        f\"\\nLogit diffs for incorrect answer type: '{idx_to_incorrect_answer_label[incorrect_idx]}':\"\n",
    "    )\n",
    "    logit_diffs = logits_to_ave_logit_diff_2(\n",
    "        logits, answer_tokens, per_prompt=True, incorrect_idx=incorrect_idx\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "- All values high now. Avoid sentences that are expected to break standard English prose rule?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_idx = 1\n",
    "\n",
    "original_average_logit_diff = logits_to_ave_logit_diff_2(logits, answer_tokens, per_prompt=False, incorrect_idx=incorrect_idx, print_=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer residual directions shape: torch.Size([8, 4, 768])\n",
      "Logit difference directions shape: torch.Size([8, 768])\n",
      "Final residual stream shape: torch.Size([8, 11, 768])\n",
      "Calculated average logit diff: 7.14\n",
      "Original logit difference: 6.453\n"
     ]
    }
   ],
   "source": [
    "answer_residual_directions = model.tokens_to_residual_directions(answer_tokens)\n",
    "print(\"Answer residual directions shape:\", answer_residual_directions.shape)\n",
    "\n",
    "logit_diff_directions = (\n",
    "    answer_residual_directions[:, 0] - answer_residual_directions[:, incorrect_idx]\n",
    ")\n",
    "print(\"Logit difference directions shape:\", logit_diff_directions.shape)\n",
    "\n",
    "# cache syntax - resid_post is the residual stream at the end of the layer, -1 gets the final layer. The general syntax is [activation_name, layer_index, sub_layer_type].\n",
    "final_residual_stream = cache[\"resid_post\", -1]\n",
    "print(\"Final residual stream shape:\", final_residual_stream.shape)\n",
    "final_token_residual_stream = final_residual_stream[:, -1, :]\n",
    "# Apply LayerNorm scaling\n",
    "# pos_slice is the subset of the positions we take - here the final token of each prompt\n",
    "scaled_final_token_residual_stream = cache.apply_ln_to_stack(\n",
    "    final_token_residual_stream, layer=-1, pos_slice=-1\n",
    ")\n",
    "\n",
    "average_logit_diff = einsum(\n",
    "    \"batch d_model, batch d_model -> \",\n",
    "    scaled_final_token_residual_stream,\n",
    "    logit_diff_directions,\n",
    ") / len(prompts)\n",
    "print(\"Calculated average logit diff:\", round(average_logit_diff.item(), 3))\n",
    "print(\"Original logit difference:\", round(original_average_logit_diff.item(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Still different.\n",
    "- I think a possible cause is that I have not not been picking the top answer for each prompt\n",
    "as the correct answer.\n",
    "\n",
    "  - E.g., there may be some difference in scaling across the prompts that makes comparing\n",
    "an average invalid. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1514,  467],\n",
      "        [ 198, 1312],\n",
      "        [ 198, 1312],\n",
      "        [ 198, 1312],\n",
      "        [ 314, 1312],\n",
      "        [ 632,  340],\n",
      "        [ 198, 1312],\n",
      "        [ 198, 1312]], device='cuda:0')\n",
      "Answer residual directions shape: torch.Size([8, 2, 768])\n",
      "Logit difference directions shape: torch.Size([8, 768])\n",
      "Final residual stream shape: torch.Size([8, 2, 768])\n",
      "Calculated average logit diff: -1.614\n",
      "Original logit difference: 0.233\n"
     ]
    }
   ],
   "source": [
    "top_answers = [\n",
    "    (\" Go\", \" go\"),\n",
    "    (\"\\n\", \" i\"),\n",
    "    (\"\\n\", \" i\"),\n",
    "    (\"\\n\", \" i\"),\n",
    "    (\" I\", \" i\"),\n",
    "    (\" It\", \" it\"),\n",
    "    (\"\\n\", \" i\"),\n",
    "    (\"\\n\", \" i\"),\n",
    "]\n",
    "\n",
    "top_answer_tokens = torch.tensor(\n",
    "    [[model.to_single_token(ans_row[0]), model.to_single_token(ans_row[1])] for ans_row in top_answers]\n",
    ").to(device)\n",
    "print(top_answer_tokens)\n",
    "logits_top_answer, cache_top_answer = model.run_with_cache(top_answer_tokens)\n",
    "original_average_logit_diff = logits_to_ave_logit_diff_2(logits_top_answer, top_answer_tokens, per_prompt=False, incorrect_idx=1, print_=False)\n",
    "\n",
    "answer_residual_directions = model.tokens_to_residual_directions(top_answer_tokens)\n",
    "print(\"Answer residual directions shape:\", answer_residual_directions.shape)\n",
    "\n",
    "top_answer_logit_diff_directions = (\n",
    "    answer_residual_directions[:, 0] - answer_residual_directions[:, 1]\n",
    ")\n",
    "print(\"Logit difference directions shape:\", top_answer_logit_diff_directions.shape)\n",
    "\n",
    "# cache syntax - resid_post is the residual stream at the end of the layer, -1 gets the final layer. The general syntax is [activation_name, layer_index, sub_layer_type].\n",
    "final_residual_stream = cache_top_answer[\"resid_post\", -1]\n",
    "print(\"Final residual stream shape:\", final_residual_stream.shape)\n",
    "final_token_residual_stream = final_residual_stream[:, -1, :]\n",
    "# Apply LayerNorm scaling\n",
    "# pos_slice is the subset of the positions we take - here the final token of each prompt\n",
    "scaled_final_token_residual_stream = cache_top_answer.apply_ln_to_stack(\n",
    "    final_token_residual_stream, layer=-1, pos_slice=-1\n",
    ")\n",
    "\n",
    "average_logit_diff = einsum(\n",
    "    \"batch d_model, batch d_model -> \",\n",
    "    scaled_final_token_residual_stream,\n",
    "    top_answer_logit_diff_directions,\n",
    ") / len(top_answers)\n",
    "print(\"Calculated average logit diff:\", round(average_logit_diff.item(), 3))\n",
    "print(\"Original logit difference:\", round(original_average_logit_diff.item(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm. Still no good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the interest of time, press on\n",
    "\n",
    "- In normal circumstances, I would seek advice from a colleague/mentor on cause/importance of this difference in scaling.\n",
    "\n",
    "- I have a small hunch that the difference may not overly adversely impact meaningfulness of results.\n",
    "\n",
    "- Press on as if no difference for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_average_logit_diff = logits_to_ave_logit_diff_2(logits, answer_tokens, per_prompt=False, incorrect_idx=1, print_=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_stack_to_logit_diff(\n",
    "    residual_stack: Float[torch.Tensor, \"components batch d_model\"],\n",
    "    cache: ActivationCache,\n",
    ") -> float:\n",
    "    scaled_residual_stack = cache.apply_ln_to_stack(\n",
    "        residual_stack, layer=-1, pos_slice=-1\n",
    "    )\n",
    "    return einsum(\n",
    "        \"... batch d_model, batch d_model -> ...\",\n",
    "        scaled_residual_stack,\n",
    "        logit_diff_directions,\n",
    "    ) / len(prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "<b>%{hovertext}</b><br><br>x=%{x}<br>y=%{y}<extra></extra>",
         "hovertext": [
          "0_pre",
          "0_mid",
          "1_pre",
          "1_mid",
          "2_pre",
          "2_mid",
          "3_pre",
          "3_mid",
          "4_pre",
          "4_mid",
          "5_pre",
          "5_mid",
          "6_pre",
          "6_mid",
          "7_pre",
          "7_mid",
          "8_pre",
          "8_mid",
          "9_pre",
          "9_mid",
          "10_pre",
          "10_mid",
          "11_pre",
          "11_mid",
          "final_post"
         ],
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0,
          0.5,
          1,
          1.5,
          2,
          2.5,
          3,
          3.5,
          4,
          4.5,
          5,
          5.5,
          6,
          6.5,
          7,
          7.5,
          8,
          8.5,
          9,
          9.5,
          10,
          10.5,
          11,
          11.5,
          12
         ],
         "xaxis": "x",
         "y": [
          0.006045743823051453,
          0.3832247257232666,
          0.8227099776268005,
          0.8222269415855408,
          0.9625974297523499,
          0.966791033744812,
          1.0316193103790283,
          1.0313239097595215,
          1.162224292755127,
          1.1547267436981201,
          1.2580480575561523,
          1.3580373525619507,
          1.4630929231643677,
          1.4921249151229858,
          1.794796347618103,
          1.8556709289550781,
          2.268353223800659,
          2.3155112266540527,
          2.9885880947113037,
          3.1024842262268066,
          3.922022581100464,
          4.0002827644348145,
          4.986746311187744,
          5.366740703582764,
          7.1402506828308105
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Logit Difference From Accumulated Residual Stream"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "x"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "y"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accumulated_residual, labels = cache.accumulated_resid(\n",
    "    layer=-1, incl_mid=True, pos_slice=-1, return_labels=True\n",
    ")\n",
    "logit_lens_logit_diffs = residual_stack_to_logit_diff(accumulated_residual, cache)\n",
    "line(\n",
    "    logit_lens_logit_diffs,\n",
    "    x=np.arange(model.cfg.n_layers * 2 + 1) / 2,\n",
    "    hover_name=labels,\n",
    "    title=\"Logit Difference From Accumulated Residual Stream\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "<b>%{hovertext}</b><br><br>x=%{x}<br>y=%{y}<extra></extra>",
         "hovertext": [
          "embed",
          "pos_embed",
          "0_attn_out",
          "0_mlp_out",
          "1_attn_out",
          "1_mlp_out",
          "2_attn_out",
          "2_mlp_out",
          "3_attn_out",
          "3_mlp_out",
          "4_attn_out",
          "4_mlp_out",
          "5_attn_out",
          "5_mlp_out",
          "6_attn_out",
          "6_mlp_out",
          "7_attn_out",
          "7_mlp_out",
          "8_attn_out",
          "8_mlp_out",
          "9_attn_out",
          "9_mlp_out",
          "10_attn_out",
          "10_mlp_out",
          "11_attn_out",
          "11_mlp_out"
         ],
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25
         ],
         "xaxis": "x",
         "y": [
          0.004200339317321777,
          0.00184540543705225,
          0.37717893719673157,
          0.4394853711128235,
          -0.0004830812104046345,
          0.1403704583644867,
          0.004193556495010853,
          0.0648283064365387,
          -0.00029546557925641537,
          0.13090047240257263,
          -0.0074976044707000256,
          0.1033213809132576,
          0.09998930245637894,
          0.10505566745996475,
          0.029031962156295776,
          0.3026711344718933,
          0.06087451055645943,
          0.4126826226711273,
          0.047157786786556244,
          0.6730771064758301,
          0.11389625072479248,
          0.8195381760597229,
          0.07825981825590134,
          0.9864639043807983,
          0.37999430298805237,
          1.7735100984573364
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Logit Difference From Each Layer"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "x"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "y"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "per_layer_residual, labels = cache.decompose_resid(\n",
    "    layer=-1, pos_slice=-1, return_labels=True\n",
    ")\n",
    "per_layer_logit_diffs = residual_stack_to_logit_diff(per_layer_residual, cache)\n",
    "line(per_layer_logit_diffs, hover_name=labels, title=\"Logit Difference From Each Layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "\n",
    "- It appears that MLP layers are primarily what matters.\n",
    "\n",
    "- Especially the later ones (with increasing effect toward the later layers)\n",
    "\n",
    "- With my limited understanding of transformers, this makes some sense. It seems to me that the most important information for predicting a space-titleword token next is the nature of the *current* position/token: it is a \"sentence terminating character\". Since MLP layers are used to process information at a position, I think this info is likely \"determined/used\" in MLP layera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head Attribution\n",
    "\n",
    "- Just out of interest, of course heads are parts of attention layers, which are apparently relatively unimportant - at least directly - according to the layer attribution graph above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to stack head results when they weren't cached. Computing head results now\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "Head: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "xaxis": "x",
         "yaxis": "y",
         "z": [
          [
           -0.0014938986860215664,
           0.13968972861766815,
           0.051865458488464355,
           -0.021638432517647743,
           0.03641578555107117,
           -0.011396054178476334,
           0.003805173560976982,
           -0.0052239010110497475,
           0.0640220195055008,
           0.07961846888065338,
           -0.011879147961735725,
           0.07643289119005203
          ],
          [
           0.003184700384736061,
           0.01621519774198532,
           -0.009807128459215164,
           -0.05302409827709198,
           -0.0066294483840465546,
           -0.01596742682158947,
           0.02402934432029724,
           -0.029515597969293594,
           0.024050267413258553,
           -0.00029715243726968765,
           0.04122340306639671,
           0.013555868528783321
          ],
          [
           0.016342859715223312,
           0.006091217510402203,
           0.007417414337396622,
           0.03459965065121651,
           0.005218225531280041,
           0.02120096981525421,
           -0.016758915036916733,
           -0.000031053321436047554,
           -0.040174681693315506,
           -0.005849276203662157,
           0.0331154391169548,
           -0.03754034638404846
          ],
          [
           -0.007055611349642277,
           0.011210334487259388,
           -0.010543654672801495,
           0.002857946092262864,
           0.0127693647518754,
           0.008187944069504738,
           0.007670317776501179,
           -0.013586434535682201,
           0.00955929048359394,
           -0.0178139079362154,
           -0.0055366745218634605,
           0.007116546388715506
          ],
          [
           -0.000022641383111476898,
           0.008579779416322708,
           0.013949668034911156,
           -0.002847732976078987,
           -0.007296031806617975,
           0.00047748791985213757,
           0.009718409739434719,
           -0.00861334428191185,
           -0.020505432039499283,
           0.0013998711947351694,
           -0.030341599136590958,
           -0.01299980003386736
          ],
          [
           0.027127321809530258,
           0.009810203686356544,
           -0.023552048951387405,
           0.028572235256433487,
           0.0023444960825145245,
           0.004866709001362324,
           -0.00030669476836919785,
           0.003629095619544387,
           0.015889111906290054,
           -0.0024542827159166336,
           0.013718429021537304,
           0.0027631442062556744
          ],
          [
           0.012586550787091255,
           -0.023649467155337334,
           -0.01778758317232132,
           0.03553362935781479,
           -0.006383370142430067,
           0.03994297981262207,
           -0.03589501604437828,
           0.0008054864592850208,
           -0.024975385516881943,
           0.07118445634841919,
           -0.02967708930373192,
           0.008445590734481812
          ],
          [
           -0.01522522047162056,
           0.023592522367835045,
           0.006095473654568195,
           0.0013961773365736008,
           0.0014202874153852463,
           0.007191944867372513,
           -0.029576608911156654,
           -0.007732166908681393,
           0.009334776550531387,
           0.007915428839623928,
           0.01366115640848875,
           0.00023310072720050812
          ],
          [
           -0.025818677619099617,
           -0.009494679048657417,
           0.03443802520632744,
           0.03828265890479088,
           0.010000464506447315,
           -0.027104929089546204,
           -0.003413938218727708,
           -0.011898251250386238,
           0.003025502897799015,
           0.010870951227843761,
           -0.019112039357423782,
           0.016133205965161324
          ],
          [
           0.020969213917851448,
           0.0029781078919768333,
           -0.011268899776041508,
           -0.031218767166137695,
           0.028546325862407684,
           -0.04408768564462662,
           0.05492280796170235,
           0.019870737567543983,
           -0.004012224730104208,
           0.029311448335647583,
           -0.010277105495333672,
           -0.01068611815571785
          ],
          [
           0.016249025240540504,
           0.04799671471118927,
           0.06201693043112755,
           -0.015740498900413513,
           -0.016057083383202553,
           0.017119163647294044,
           0.02099880576133728,
           -0.11150235682725906,
           -0.007382892072200775,
           -0.035176098346710205,
           0.04291561245918274,
           0.003111638128757477
          ],
          [
           0.3661387264728546,
           0.01851698011159897,
           0.010484859347343445,
           0.06106368079781532,
           -0.020444661378860474,
           0.015306377783417702,
           -0.002700790762901306,
           -0.04678390175104141,
           -0.04707641154527664,
           0.01908768340945244,
           -0.03684485703706741,
           0.018260616809129715
          ]
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "cmid": 0,
         "colorscale": [
          [
           0,
           "rgb(103,0,31)"
          ],
          [
           0.1,
           "rgb(178,24,43)"
          ],
          [
           0.2,
           "rgb(214,96,77)"
          ],
          [
           0.3,
           "rgb(244,165,130)"
          ],
          [
           0.4,
           "rgb(253,219,199)"
          ],
          [
           0.5,
           "rgb(247,247,247)"
          ],
          [
           0.6,
           "rgb(209,229,240)"
          ],
          [
           0.7,
           "rgb(146,197,222)"
          ],
          [
           0.8,
           "rgb(67,147,195)"
          ],
          [
           0.9,
           "rgb(33,102,172)"
          ],
          [
           1,
           "rgb(5,48,97)"
          ]
         ]
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Logit Difference From Each Head"
        },
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "scaleanchor": "y",
         "title": {
          "text": "Head"
         }
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Layer"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "per_head_residual, labels = cache.stack_head_results(\n",
    "    layer=-1, pos_slice=-1, return_labels=True\n",
    ")\n",
    "per_head_logit_diffs = residual_stack_to_logit_diff(per_head_residual, cache)\n",
    "per_head_logit_diffs = einops.rearrange(\n",
    "    per_head_logit_diffs,\n",
    "    \"(layer head_index) -> layer head_index\",\n",
    "    layer=model.cfg.n_layers,\n",
    "    head_index=model.cfg.n_heads,\n",
    ")\n",
    "imshow(\n",
    "    per_head_logit_diffs,\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    "    title=\"Logit Difference From Each Head\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "- `0.1`, `10.7` and `11.0` look most interesting, in at least relative to the others.\n",
    "\n",
    "- Note the colormap  scale. Not much effect - at least as compared to IOI! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_patterns(\n",
    "    heads: Union[List[int], int, Float[torch.Tensor, \"heads\"]],\n",
    "    local_cache: ActivationCache,\n",
    "    local_tokens: torch.Tensor,\n",
    "    title: Optional[str] = \"\",\n",
    "    max_width: Optional[int] = 700,\n",
    ") -> str:\n",
    "    # If a single head is given, convert to a list\n",
    "    if isinstance(heads, int):\n",
    "        heads = [heads]\n",
    "\n",
    "    # Create the plotting data\n",
    "    labels: List[str] = []\n",
    "    patterns: List[Float[torch.Tensor, \"dest_pos src_pos\"]] = []\n",
    "\n",
    "    # Assume we have a single batch item\n",
    "    batch_index = 0\n",
    "\n",
    "    for head in heads:\n",
    "        # Set the label\n",
    "        layer = head // model.cfg.n_heads\n",
    "        head_index = head % model.cfg.n_heads\n",
    "        labels.append(f\"L{layer}H{head_index}\")\n",
    "\n",
    "        # Get the attention patterns for the head\n",
    "        # Attention patterns have shape [batch, head_index, query_pos, key_pos]\n",
    "        patterns.append(local_cache[\"attn\", layer][batch_index, head_index])\n",
    "\n",
    "    # Convert the tokens to strings (for the axis labels)\n",
    "    str_tokens = model.to_str_tokens(local_tokens)\n",
    "\n",
    "    # Combine the patterns into a single tensor\n",
    "    patterns: Float[torch.Tensor, \"head_index dest_pos src_pos\"] = torch.stack(\n",
    "        patterns, dim=0\n",
    "    )\n",
    "\n",
    "    # Circuitsvis Plot (note we get the code version so we can concatenate with the title)\n",
    "    plot = attention_heads(\n",
    "        attention=patterns, tokens=str_tokens, attention_head_names=labels\n",
    "    ).show_code()\n",
    "\n",
    "    # Display the title\n",
    "    title_html = f\"<h2>{title}</h2><br/>\"\n",
    "\n",
    "    # Return the visualisation as raw code\n",
    "    return f\"<div style='max-width: {str(max_width)}px;'>{title_html + plot}</div>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='max-width: 700px;'><h2>Top 3 Positive Logit Attribution Heads</h2><br/><div id=\"circuits-vis-6b9f92e3-42c1\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, AttentionHeads } from \"https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-6b9f92e3-42c1\",\n",
       "      AttentionHeads,\n",
       "      {\"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8072689771652222, 0.19273103773593903, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.39606568217277527, 0.34098777174949646, 0.2629465162754059, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3663756251335144, 0.21596617996692657, 0.16522450745105743, 0.2524336874485016, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.41715314984321594, 0.07133759558200836, 0.1765328198671341, 0.18711544573307037, 0.14786097407341003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2636759877204895, 0.10417738556861877, 0.12840785086154938, 0.16137582063674927, 0.2307903915643692, 0.11157253384590149, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2513483762741089, 0.03102060966193676, 0.1118660718202591, 0.12479223310947418, 0.16409258544445038, 0.06572666019201279, 0.25115346908569336, 0.0, 0.0, 0.0, 0.0], [0.21268317103385925, 0.03146596997976303, 0.05042346194386482, 0.03470305725932121, 0.0344177782535553, 0.07105568796396255, 0.2012922316789627, 0.3639586567878723, 0.0, 0.0, 0.0], [0.4236028790473938, 0.08953793346881866, 0.03296317905187607, 0.059110891073942184, 0.04823220148682594, 0.029885629191994667, 0.06305036693811417, 0.20218731462955475, 0.05142956227064133, 0.0, 0.0], [0.38477569818496704, 0.030375583097338676, 0.0950409397482872, 0.06556392461061478, 0.06274737417697906, 0.044236667454242706, 0.10454637557268143, 0.052109021693468094, 0.04355261102318764, 0.11705182492733002, 0.0], [0.279458224773407, 0.02987462840974331, 0.038733407855033875, 0.029440609738230705, 0.018166588619351387, 0.05895372852683067, 0.11027822643518448, 0.14086852967739105, 0.10552056133747101, 0.08296068757772446, 0.10574482381343842]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0003789146721828729, 0.9996210336685181, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [7.339733565459028e-05, 0.00249064271338284, 0.997435986995697, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.000272715522442013, 0.0006405745516531169, 0.00042986171320080757, 0.9986567497253418, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0031309062615036964, 0.005285950377583504, 0.0005039328243583441, 0.0392005555331707, 0.9518786072731018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.5006581634224858e-05, 6.465916521847248e-05, 7.871762500144541e-05, 0.000309374911012128, 1.9874385543516837e-05, 0.9995124340057373, 0.0, 0.0, 0.0, 0.0, 0.0], [2.8403410397004336e-05, 0.00016516898176632822, 0.00024424202274531126, 0.00022291061759460717, 0.0002609155490063131, 5.128303382662125e-05, 0.9990271329879761, 0.0, 0.0, 0.0, 0.0], [0.01459337119013071, 0.0007895631133578718, 0.0002272736164741218, 0.0016011694679036736, 0.008408202789723873, 0.00022517700563184917, 0.0001214134317706339, 0.974033772945404, 0.0, 0.0, 0.0], [0.12343044579029083, 0.002977535827085376, 0.00016131121083162725, 0.002607922535389662, 0.015600915998220444, 0.0005669486126862466, 0.00028066179947927594, 0.029705077409744263, 0.8246690630912781, 0.0, 0.0], [0.0004206265730317682, 0.0005183183820918202, 1.9357255951035768e-05, 0.002515322295948863, 0.0002679186291061342, 9.628591942600906e-05, 4.139124575885944e-05, 9.848583431448787e-05, 0.00015695048205088824, 0.9958653450012207, 0.0], [0.006992611102759838, 0.00026965048164129257, 9.33531773625873e-05, 0.0005566492327488959, 0.003398501081392169, 9.012317605083808e-05, 4.88293262606021e-05, 0.4936283528804779, 0.0007231304189190269, 0.00018264535174239427, 0.494016170501709]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8863518238067627, 0.11364816874265671, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8223246932029724, 0.12285574525594711, 0.05481952801346779, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6597962975502014, 0.13186155259609222, 0.09022696316242218, 0.11811515688896179, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5374982953071594, 0.10205129534006119, 0.07093988358974457, 0.11689736694097519, 0.17261311411857605, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5872894525527954, 0.0934605747461319, 0.06371767073869705, 0.09478840976953506, 0.1269172728061676, 0.033826541155576706, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5791822671890259, 0.07336998730897903, 0.07208143174648285, 0.0971742793917656, 0.1106189638376236, 0.06253305822610855, 0.005039966199547052, 0.0, 0.0, 0.0, 0.0], [0.41226157546043396, 0.08403193950653076, 0.06497921049594879, 0.08240451663732529, 0.1135256290435791, 0.08618047833442688, 0.051633309572935104, 0.10498332232236862, 0.0, 0.0, 0.0], [0.39365407824516296, 0.07591336965560913, 0.046394579112529755, 0.07028605043888092, 0.09299015253782272, 0.06505908071994781, 0.047869399189949036, 0.0902475044131279, 0.11758583039045334, 0.0, 0.0], [0.41130104660987854, 0.07555819302797318, 0.052536338567733765, 0.06240198016166687, 0.09075046330690384, 0.056925609707832336, 0.04081985354423523, 0.08629385381937027, 0.09569065272808075, 0.02772204950451851, 0.0], [0.3104654848575592, 0.06304948031902313, 0.049866124987602234, 0.06299315392971039, 0.08474995195865631, 0.06870544701814651, 0.043538715690374374, 0.08206064254045486, 0.09618567675352097, 0.052229367196559906, 0.086155965924263]]], \"attentionHeadNames\": [\"L11H0\", \"L0H1\", \"L0H9\"], \"tokens\": [\"<|endoftext|>\", \"F\", \"illing\", \" up\", \" to\", \" eleven\", \" tokens\", \".\", \"\\n\", \"Go\", \".\"], \"maskUpperTri\": true}\n",
       "    )\n",
       "    </script></div><div style='max-width: 700px;'><h2>Top 3 Negative Logit Attribution Heads</h2><br/><div id=\"circuits-vis-a428ea97-4783\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, AttentionHeads } from \"https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-a428ea97-4783\",\n",
       "      AttentionHeads,\n",
       "      {\"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9896830320358276, 0.010316967964172363, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9097660779953003, 0.007996486499905586, 0.08223740756511688, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.911576509475708, 0.005346783436834812, 0.049682892858982086, 0.03339386731386185, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9024960398674011, 0.003855999791994691, 0.018018275499343872, 0.02796141617000103, 0.0476682223379612, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.823502779006958, 0.008665203116834164, 0.020909957587718964, 0.02021244913339615, 0.0855812281370163, 0.04112840071320534, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8905805349349976, 0.0012030750513076782, 0.025259073823690414, 0.015033013187348843, 0.01411410141736269, 0.0067208451218903065, 0.04708944633603096, 0.0, 0.0, 0.0, 0.0], [0.6700867414474487, 0.021110989153385162, 0.04497728496789932, 0.013027379289269447, 0.029727431014180183, 0.01465862337499857, 0.11803118884563446, 0.08838041126728058, 0.0, 0.0, 0.0], [0.8667372465133667, 0.017448168247938156, 0.025347502902150154, 0.004399971570819616, 0.009580415673553944, 0.012228071689605713, 0.022956132888793945, 0.023709993809461594, 0.017592409625649452, 0.0, 0.0], [0.9223873019218445, 0.0013068616390228271, 0.005331001244485378, 0.016684364527463913, 0.01177915371954441, 0.0035071424208581448, 0.013923993334174156, 0.00856175646185875, 0.006855495274066925, 0.009662853553891182, 0.0], [0.5346782207489014, 0.007940990850329399, 0.007380958646535873, 0.013646593317389488, 0.019117776304483414, 0.0009748418815433979, 0.0066255684942007065, 0.024295520037412643, 0.01595330238342285, 0.3166271150112152, 0.05275912210345268]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6402127742767334, 0.3597872257232666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6083611249923706, 0.1554824858903885, 0.23615635931491852, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5571038722991943, 0.11275038868188858, 0.1664961874485016, 0.16364963352680206, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5322960615158081, 0.08492660522460938, 0.1233275979757309, 0.1219213679432869, 0.1375284343957901, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5030842423439026, 0.06465210020542145, 0.09800052642822266, 0.10096723586320877, 0.10966020822525024, 0.1236356869339943, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5158265233039856, 0.05033392086625099, 0.07370569556951523, 0.07643386721611023, 0.08517623692750931, 0.0936669260263443, 0.10485684871673584, 0.0, 0.0, 0.0, 0.0], [0.4881276786327362, 0.041345324367284775, 0.06142343953251839, 0.06457295268774033, 0.06967658549547195, 0.07961196452379227, 0.09200466424226761, 0.10323739796876907, 0.0, 0.0, 0.0], [0.5157991051673889, 0.03431041166186333, 0.04698682203888893, 0.05065983906388283, 0.053846586495637894, 0.06162455305457115, 0.07109080255031586, 0.07982474565505981, 0.08585712313652039, 0.0, 0.0], [0.4730377197265625, 0.026124071329832077, 0.04217160865664482, 0.046211812645196915, 0.05105584114789963, 0.058182720094919205, 0.07158654183149338, 0.07934290915727615, 0.08633682131767273, 0.0659499242901802, 0.0], [0.493515282869339, 0.022209173068404198, 0.035175472497940063, 0.038810670375823975, 0.04241131991147995, 0.04732691869139671, 0.05801001191139221, 0.06369880586862564, 0.07061409950256348, 0.05646407604217529, 0.07176416367292404]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.6158908692887053e-05, 0.9999837875366211, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [7.447151438100263e-05, 0.2990161180496216, 0.7009094953536987, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0448620741954073e-05, 0.03809225559234619, 0.7453977465629578, 0.2164994776248932, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [3.822132566710934e-05, 0.07569266855716705, 0.3964516520500183, 0.2907618582248688, 0.23705556988716125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [6.234633474377915e-05, 0.024423930794000626, 0.31949862837791443, 0.249970480799675, 0.2933807373046875, 0.11266382783651352, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00017571081116329879, 0.01200109627097845, 0.14916783571243286, 0.16081871092319489, 0.11593758314847946, 0.10846617072820663, 0.4534328877925873, 0.0, 0.0, 0.0, 0.0], [3.92253277823329e-05, 0.01687263324856758, 0.023393522948026657, 0.029201894998550415, 0.04702307656407356, 0.029026314616203308, 0.41538330912590027, 0.4390599727630615, 0.0, 0.0, 0.0], [4.4625943701248616e-05, 0.06790554523468018, 0.0385015532374382, 0.021173741668462753, 0.06422211974859238, 0.043039560317993164, 0.4039727747440338, 0.094691202044487, 0.26644882559776306, 0.0, 0.0], [3.9511785871582106e-05, 0.08310185372829437, 0.06755661219358444, 0.06778270751237869, 0.12312672287225723, 0.0479118674993515, 0.12547047436237335, 0.13230925798416138, 0.20652955770492554, 0.1461714208126068, 0.0], [3.69819208572153e-05, 0.07067035883665085, 0.06136611849069595, 0.05778883770108223, 0.05082797259092331, 0.03761334717273712, 0.08099835366010666, 0.13220776617527008, 0.09830550104379654, 0.18730495870113373, 0.22287976741790771]]], \"attentionHeadNames\": [\"L10H7\", \"L1H3\", \"L11H8\"], \"tokens\": [\"<|endoftext|>\", \"F\", \"illing\", \" up\", \" to\", \" eleven\", \" tokens\", \".\", \"\\n\", \"Go\", \".\"], \"maskUpperTri\": true}\n",
       "    )\n",
       "    </script></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k = 3\n",
    "\n",
    "top_positive_logit_attr_heads = torch.topk(\n",
    "    per_head_logit_diffs.flatten(), k=top_k\n",
    ").indices\n",
    "\n",
    "positive_html = visualize_attention_patterns(\n",
    "    top_positive_logit_attr_heads,\n",
    "    cache,\n",
    "    tokens[0],\n",
    "    f\"Top {top_k} Positive Logit Attribution Heads\",\n",
    ")\n",
    "\n",
    "top_negative_logit_attr_heads = torch.topk(\n",
    "    -per_head_logit_diffs.flatten(), k=top_k\n",
    ").indices\n",
    "\n",
    "negative_html = visualize_attention_patterns(\n",
    "    top_negative_logit_attr_heads,\n",
    "    cache,\n",
    "    tokens[0],\n",
    "    title=f\"Top {top_k} Negative Logit Attribution Heads\",\n",
    ")\n",
    "\n",
    "HTML(positive_html + negative_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "- In the strongest head, `11.0`, token `1` attends strongly (relatively speaking) to token `0` (`<|endoftext|>`). This might contribute to capitalisation of token `1`?\n",
    "\n",
    "- In `0.1`, the final token (`\".\"`) attends fairly strongly to token `7`, which is another full stop in at least one of the prompts.\n",
    "\n",
    "- In `10.7`, attention to the second to last token negatively contributes to the logits of the final token.\n",
    "  - One (major?) flaw in this analysis is investigating only the first titleword token. For the selection of prompts I chose, perhaps the token immediately preceding the final full stop contributes to a stronger prediction of some other token (the newline?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Stream Patching\n",
    "\n",
    "- **Note**: It's difficult to for me to know how much wisdom I can draw from this section. The \"corrupted\" prompts below do significantly increase the logits for the \\<space\\>\\<**lowercase**\\> version of the next token, but they do not meaningfully *reduce* the logits of the \"clean\" token. It is not a neat reversal like the IOI example\n",
    "\n",
    "- The problem is: I'm struggling to think of a way to manufacture such a reversal, especially using the *relevant circuitry*. I'm sure I am missing something, but this seems like a difficult problem to provide an input that tests the \"capitalise next word after full stop\" circuit for which I can provide a corrupted input that produces an uncapitalised first word.\n",
    "  - E.g., perhaps I could exclusively use **repeat sequences** of a single word (e.g., clean version:`[\"Go. Go. Go. Go\"]`, corrupted version:`[\"go. go. go. go\"]`), but it seems likely that some *different circuit* (e.g., duplication) produces capitalised output in the clean case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per prompt logit difference: tensor([ 1.3570, -2.6480, -1.0820,  3.3580,  2.7450,  2.0550,  2.8190, -2.4030])\n",
      "Average logit difference: 0.775\n",
      "\n",
      "Corrupted Average Logit Diff 0.78\n",
      "Clean Average Logit Diff 6.45\n",
      "Tokenized prompt: ['<|endoftext|>', 'hello', '.', ' hello', '.', ' hello', '.', ' hello', '.', ' hello', '.']\n",
      "Tokenized answer: [' Hello']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11.44</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.51</span><span style=\"font-weight: bold\">% Token: | Hello|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m11.44\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m2.51\u001b[0m\u001b[1m% Token: | Hello|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.09 Prob: 35.49% Token: | hello|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Hello'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Hello'\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'hello', '.', ' hello', '.', ' hello', '.', ' hello', '.', ' hello', '.']\n",
      "Tokenized answer: [' hello']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.09</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35.49</span><span style=\"font-weight: bold\">% Token: | hello|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m14.09\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m35.49\u001b[0m\u001b[1m% Token: | hello|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.09 Prob: 35.49% Token: | hello|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' hello'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' hello'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompts_corrupted = [\n",
    "    \"filling up to eleven tokens.\\ngo.\", # Single word sentence\n",
    "    \"hello. hello. hello. hello. hello.\", # Repeat single word sentence\n",
    "    \"yeah Matt doesn't know what he is doing.\", # More or less standard sentence\n",
    "    \"that Will does not know where he is going.\", # More or less standard sentence, different end verb\n",
    "    \"someone should really help them out, I think.\", # More or less standard sentence, not \"ing\" end verb, has comma\n",
    "    \"it is wonderful to be in Adelaide in March.\", # More or less standard sentence, end noun, has comma\n",
    "    \"koalas are cute, but grumpy.\", # More or less standard sentence, end in adjective\n",
    "    \"this is a sentence. this is another sentence.\", # Two sentences\n",
    "]\n",
    "\n",
    "tokens_corrupted = model.to_tokens(prompts_corrupted)\n",
    "\n",
    "logits_corrupted, cache_corrupted = model.run_with_cache(tokens_corrupted)\n",
    "logits_final_corrupted = logits_corrupted[:, -1, :]\n",
    "logits_sorted_corrupted, logits_idx_sorted_corrupted = logits_final_corrupted.sort(\n",
    "    descending=True, stable=True, dim=-1\n",
    ")\n",
    "average_logit_diff_corrupted = logits_to_ave_logit_diff_2(logits_corrupted, answer_tokens, per_prompt=False, incorrect_idx=1, print_=True)\n",
    "print(\"\\nCorrupted Average Logit Diff\", round(average_logit_diff_corrupted.item(), 2))\n",
    "print(\"Clean Average Logit Diff\", round(original_average_logit_diff.item(), 2))\n",
    "utils.test_prompt(prompts_corrupted[1], answer_str_tokens[1][0], model, top_k=1)\n",
    "utils.test_prompt(prompts_corrupted[1], answer_str_tokens[1][1], model, top_k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "\n",
    "- 5 out of 8 did not even reverse! But any positive logit differences are smaller.\n",
    "\n",
    "- That said, at least for these prompts, ***capitalisation of the next \"word-like\" token appears to depend fairly strongly on whether or not the *first* token (not token `0`/`<|endoftext|>`) was capitalised.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_residual_component(\n",
    "    corrupted_residual_component: Float[torch.Tensor, \"batch pos d_model\"],\n",
    "    hook,\n",
    "    pos,\n",
    "    clean_cache,\n",
    "):\n",
    "    corrupted_residual_component[:, pos, :] = clean_cache[hook.name][:, pos, :]\n",
    "    return corrupted_residual_component\n",
    "\n",
    "\n",
    "def normalize_patched_logit_diff(patched_logit_diff):\n",
    "    # Subtract corrupted logit diff to measure the improvement, divide by the total improvement from clean to corrupted to normalise\n",
    "    # 0 means zero change, negative means actively made worse, 1 means totally recovered clean performance, >1 means actively *improved* on clean performance\n",
    "    return (patched_logit_diff - average_logit_diff_corrupted) / (\n",
    "        original_average_logit_diff - average_logit_diff_corrupted\n",
    "    )\n",
    "\n",
    "\n",
    "patched_residual_stream_diff = torch.zeros(\n",
    "    model.cfg.n_layers, tokens.shape[1], device=device, dtype=torch.float32\n",
    ")\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    for position in range(tokens.shape[1]):\n",
    "        hook_fn = partial(patch_residual_component, pos=position, clean_cache=cache)\n",
    "        patched_logits = model.run_with_hooks(\n",
    "            tokens_corrupted,\n",
    "            fwd_hooks=[(utils.get_act_name(\"resid_pre\", layer), hook_fn)],\n",
    "            return_type=\"logits\",\n",
    "        )\n",
    "        patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)\n",
    "\n",
    "        patched_residual_stream_diff[layer, position] = normalize_patched_logit_diff(\n",
    "            patched_logit_diff\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "Position: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "x": [
          "<|endoftext|>_0",
          "F_1",
          "illing_2",
          " up_3",
          " to_4",
          " eleven_5",
          " tokens_6",
          "._7",
          "\n_8",
          "Go_9",
          "._10"
         ],
         "xaxis": "x",
         "yaxis": "y",
         "z": [
          [
           0,
           0.5821511745452881,
           0,
           0.023243695497512817,
           0,
           0.0281058456748724,
           0.08453238010406494,
           0.042449429631233215,
           0,
           0.13653677701950073,
           0
          ],
          [
           0,
           0.5800395011901855,
           0.004015620332211256,
           0.01984570547938347,
           0.0014104658039286733,
           0.017775794491171837,
           0.08513866364955902,
           0.03274916484951973,
           -0.00016078815679065883,
           0.13147635757923126,
           0.0019157580099999905
          ],
          [
           0,
           0.5788516402244568,
           0.02441863715648651,
           0.023181598633527756,
           0.003062239848077297,
           0.01777511276304722,
           0.08539318293333054,
           0.03404492884874344,
           -0.0005389279685914516,
           0.1337781697511673,
           0.007747940253466368
          ],
          [
           0,
           0.5671659708023071,
           0.03833431005477905,
           0.040134232491254807,
           0.005140266381204128,
           0.023218300193548203,
           0.08320295065641403,
           0.03901750221848488,
           -0.00013129913713783026,
           0.12269330769777298,
           0.019731130450963974
          ],
          [
           0,
           0.5467230081558228,
           0.04951566830277443,
           0.053165968507528305,
           0.005296770948916674,
           0.03289368376135826,
           0.07704004645347595,
           0.038350675255060196,
           0.0012763846898451447,
           0.08837412297725677,
           0.048978399485349655
          ],
          [
           0,
           0.49271321296691895,
           0.045390982180833817,
           0.05360071361064911,
           0.015166346915066242,
           0.03378443047404289,
           0.08039438724517822,
           0.03559661656618118,
           0.005639332812279463,
           0.054850105196237564,
           0.07902482897043228
          ],
          [
           0,
           0.26429110765457153,
           0.03372051939368248,
           0.05079576000571251,
           0.026735473424196243,
           0.03471575677394867,
           0.045070331543684006,
           0.03175241872668266,
           0.004404247738420963,
           0.035843320190906525,
           0.3482207953929901
          ],
          [
           0,
           0.2284768670797348,
           0.030387094244360924,
           0.030128013342618942,
           0.023487448692321777,
           0.023820899426937103,
           0.02923579327762127,
           0.023105235770344734,
           0.004816254135221243,
           0.028467504307627678,
           0.4610186219215393
          ],
          [
           0,
           0.17560739815235138,
           0.026596447452902794,
           0.02140214666724205,
           0.014528905041515827,
           0.016548531129956245,
           0.01789288967847824,
           0.016865750774741173,
           0.004336346406489611,
           0.021908123046159744,
           0.5907748341560364
          ],
          [
           0,
           0.08707857877016068,
           0.01703711971640587,
           0.013305461965501308,
           0.00841897539794445,
           0.01281371247023344,
           0.009219263680279255,
           0.009524987079203129,
           0.002968492219224572,
           0.011364792473614216,
           0.7813688516616821
          ],
          [
           0,
           0.03475034609436989,
           0.008352922275662422,
           0.005195275880396366,
           0.004405549261718988,
           0.006893052253872156,
           0.005807690322399139,
           0.004841554444283247,
           0.0020675284322351217,
           0.008834236301481724,
           0.8839426636695862
          ],
          [
           0,
           0.007639904972165823,
           0.004872009623795748,
           0.00245313229970634,
           0.0028255926445126534,
           0.004142331890761852,
           0.0030898810364305973,
           0.0016987527487799525,
           0.001216293778270483,
           0.006913733668625355,
           0.9466800689697266
          ]
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "cmid": 0,
         "colorscale": [
          [
           0,
           "rgb(103,0,31)"
          ],
          [
           0.1,
           "rgb(178,24,43)"
          ],
          [
           0.2,
           "rgb(214,96,77)"
          ],
          [
           0.3,
           "rgb(244,165,130)"
          ],
          [
           0.4,
           "rgb(253,219,199)"
          ],
          [
           0.5,
           "rgb(247,247,247)"
          ],
          [
           0.6,
           "rgb(209,229,240)"
          ],
          [
           0.7,
           "rgb(146,197,222)"
          ],
          [
           0.8,
           "rgb(67,147,195)"
          ],
          [
           0.9,
           "rgb(33,102,172)"
          ],
          [
           1,
           "rgb(5,48,97)"
          ]
         ]
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Logit Difference From Patched Residual Stream"
        },
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "scaleanchor": "y",
         "title": {
          "text": "Position"
         }
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Layer"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_position_labels = [\n",
    "    f\"{tok}_{i}\" for i, tok in enumerate(model.to_str_tokens(tokens[0]))\n",
    "]\n",
    "imshow(\n",
    "    patched_residual_stream_diff,\n",
    "    x=prompt_position_labels,\n",
    "    title=\"Logit Difference From Patched Residual Stream\",\n",
    "    labels={\"x\": \"Position\", \"y\": \"Layer\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "\n",
    "- **With all of the above caveats**\n",
    "  - It *does* appear that some of the contributory computation is happening on the first token, which is ordinarily capitalised.\n",
    "\n",
    "  - At around layers 6-8, the information is moved to the final token?\n",
    "    - Are the initial layers determining a \"capitalisation-state\" datum for the first token in the sequence, and using this to help determine the token following the full stop?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
